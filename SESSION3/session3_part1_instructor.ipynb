{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi4D4Q7leC0a"
   },
   "source": [
    "# Image Processing with Deep Learning\n",
    "<a href=\"https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2025/blob/main/SESSION3/session3_part1_instructor.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "Welcome to the third session of deep learning bootcamp.  Today we are going to learn how computers \"see\" images and how we can use deep learning for image processing.  Feel free to jot down any notes you have from today's session in this notebook and please feel free to modify and experiment with the code during today's exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: How Do Computers See Images?\n",
    "\n",
    "Before we dive into deep learning, let's understand a fundamental concept: **computers don't see images the way we do**. To a computer, every image is just a grid of numbers!\n",
    "\n",
    "Let's see this for ourselves using handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:25<00:00, 396kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 73.2kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:10<00:00, 158kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.15MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n",
      "We have 60000 images of handwritten digits to work with.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's import the tools we need\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Download the MNIST dataset (handwritten digits 0-9)\n",
    "mnist_data = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "print(\"Dataset downloaded successfully!\")\n",
    "print(f\"We have {len(mnist_data)} images of handwritten digits to work with.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing an Image as Humans Do\n",
    "\n",
    "Let's first look at an image the way we normally would:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can see this is a handwritten '5'\n"
     ]
    }
   ],
   "source": [
    "# Get one image from the dataset\n",
    "image, label = mnist_data[0]  # The first image\n",
    "\n",
    "# Display it\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"This is the digit: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"You can see this is a handwritten '{label}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing an Image as Computers Do: Just Numbers!\n",
    "\n",
    "Now let's see what the computer actually \"sees\". Each pixel (tiny dot) in the image has a brightness value from 0 (black) to 255 (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what the computer sees - a grid of numbers!\n",
      "The image is 28 pixels tall and 28 pixels wide.\n",
      "\n",
      "Here's a small portion of the numbers (top-left corner):\n",
      "[[  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36]\n",
      " [  0   0   0   0   0   0   0  49 238 253]\n",
      " [  0   0   0   0   0   0   0  18 219 253]\n",
      " [  0   0   0   0   0   0   0   0  80 156]]\n",
      "\n",
      "0 = black pixel, 255 = white pixel, values in between = shades of gray\n"
     ]
    }
   ],
   "source": [
    "# Convert the image to numbers we can look at\n",
    "image_as_numbers = (image.squeeze().numpy() * 255).astype(int)\n",
    "\n",
    "print(\"Here's what the computer sees - a grid of numbers!\")\n",
    "print(f\"The image is {image_as_numbers.shape[0]} pixels tall and {image_as_numbers.shape[1]} pixels wide.\")\n",
    "print(\"\\nHere's a small portion of the numbers (top-left corner):\")\n",
    "print(image_as_numbers[0:10, 0:10])  # Show just a 10x10 section\n",
    "print(\"\\n0 = black pixel, 255 = white pixel, values in between = shades of gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Both Views Side by Side\n",
    "\n",
    "Let's compare the image view with the number view for a small section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See how each number corresponds to how bright that pixel is!\n",
      "This is the KEY IDEA: Images are just grids of numbers to a computer.\n"
     ]
    }
   ],
   "source": [
    "# Take a small section of the image to see both views clearly\n",
    "small_section = image_as_numbers[10:18, 10:18]  # An 8x8 pixel section\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Visual representation\n",
    "ax1.imshow(small_section, cmap='gray', vmin=0, vmax=255)\n",
    "ax1.set_title('What We See (8×8 pixels)')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Right: The actual numbers\n",
    "ax2.axis('off')\n",
    "ax2.set_title('What the Computer Sees (Pixel Values)')\n",
    "table_data = [[str(val) for val in row] for row in small_section]\n",
    "table = ax2.table(cellText=table_data, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"See how each number corresponds to how bright that pixel is!\")\n",
    "print(\"This is the KEY IDEA: Images are just grids of numbers to a computer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Matters for Deep Learning\n",
    "\n",
    "Understanding that images are numbers is crucial because:\n",
    "- Computers can only do math with numbers\n",
    "- Deep learning works by doing mathematical operations on these numbers\n",
    "- When we \"process\" an image, we're really just transforming these numbers in smart ways\n",
    "\n",
    "Now let's see how we can use this understanding to detect patterns in images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avWdnFiWe9Py"
   },
   "source": [
    "## Excercise 1: Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fITfC3HAwrh"
   },
   "source": [
    "Now we will perform edge detection on an image using manually designed kernel.  Edges are where the brightness changes suddenly (like going from black to white). We can find them by comparing neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get an Image to Work With\n",
    "\n",
    "Let's pick a digit that has clear edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "ECo_5ZB37cmd",
     "outputId": "d9f32cec-56f1-48d3-e39d-4514b759f08b"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "ECo_5ZB37cmd",
     "outputId": "19df6be9-cc2a-411d-88d9-9ff5c7cbd241"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ready for edge detection!\n"
     ]
    }
   ],
   "source": [
    "# Let's find a nice clear digit to work with\n",
    "# We'll look for a '7' which has good vertical and horizontal edges\n",
    "for i in range(len(mnist_data)):\n",
    "    img, lbl = mnist_data[i]\n",
    "    if lbl == 7:\n",
    "        sample_image = img\n",
    "        sample_label = lbl\n",
    "        break\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Our Sample Digit: {sample_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Convert to the format we need for processing\n",
    "img = sample_image.float()\n",
    "print(\"Image ready for edge detection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "oTa0RB3rBdPq",
     "outputId": "9ffc4b82-2c84-43f8-f15c-ed407f0aef25"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "oTa0RB3rBdPq",
     "outputId": "b9c1971c-2d28-4f6d-88ae-660fa75fbcc0"
    }
   },
   "outputs": [],
   "source": [
    "img = Image.open('Peppers.bmp')                             # Open the image\n",
    "img = torchvision.transforms.functional.pil_to_tensor(img)  # Convert it to a tensor\n",
    "img = img.float()                                           # Convert from integer to floating point\n",
    "plt.imshow(img.squeeze().numpy(), cmap='gray')              # Use matplotlib to display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbUogKP4CT7u"
   },
   "source": [
    "Now we will detect vertical edges in the image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "W8DcbalTe4N4",
     "outputId": "572c8a23-7ddd-4c38-b35e-89851e2a25ae"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "W8DcbalTe4N4",
     "outputId": "7e9f1d23-54b2-4140-ee61-73406f27e61b"
    }
   },
   "outputs": [],
   "source": [
    "vertical_filter = torch.nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=3,\n",
    "    bias=None\n",
    ")\n",
    "\n",
    "vertical_filter.weight = torch.nn.Parameter(\n",
    "    torch.Tensor([[[\n",
    "        [-1, 0, 1],\n",
    "        [-2, 0, 2],\n",
    "        [-1, 0, 1]\n",
    "    ]]])\n",
    ")\n",
    "\n",
    "vertical_edges = vertical_filter(img.unsqueeze(0))\n",
    "vertical_edges = vertical_edges.detach().squeeze().numpy()\n",
    "plt.imshow(vertical_edges, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1TqRt2Jjl-u"
   },
   "source": [
    "Can you detect the horizontal edges?  Make a custom kernel below check the result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "V9z-rXmcjkpG",
     "outputId": "9fbb1fe0-ea34-4074-e115-ba68bc327111"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "V9z-rXmcjkpG",
     "outputId": "508ac363-db20-4a71-b750-f15abae9b9bc"
    }
   },
   "outputs": [],
   "source": [
    "horizontal_filter = torch.nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=3,\n",
    "    bias=None\n",
    ")\n",
    "\n",
    "horizontal_filter.weight = torch.nn.Parameter(\n",
    "    torch.Tensor([[[\n",
    "        [1, 2, 1],\n",
    "        [0, 0, 0],\n",
    "        [-1, -2, -1]\n",
    "    ]]])\n",
    ")\n",
    "\n",
    "horizontal_edges = horizontal_filter(img.unsqueeze(0))\n",
    "horizontal_edges = horizontal_edges.detach().squeeze().numpy()\n",
    "plt.imshow(horizontal_edges, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-tyBOa_kEJz"
   },
   "source": [
    "Now can you combine the result of the two filters together to display all edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 453
     },
     "id": "YU1wc6Ykjker",
     "outputId": "267e6c8a-1474-4154-9dac-1fad752e8f99"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 452
     },
     "id": "YU1wc6Ykjker",
     "outputId": "f28a8656-cc5c-4dd5-a4a7-b040d066a129"
    }
   },
   "outputs": [],
   "source": [
    "all_edges = horizontal_edges + vertical_edges\n",
    "plt.imshow(all_edges, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpcvHJvDb_XC"
   },
   "source": [
    "## Excercise 2: Parameters of A Convolution\n",
    "\n",
    "### 2a: input and output channels\n",
    "Define a convolutional filter with 4 input channels and 6 output channels.  What will the output shape be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "c1xCWu95cNZ9",
     "outputId": "5279ab1d-c996-4f82-a5f3-9ed575374ae5"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "c1xCWu95cNZ9",
     "outputId": "3334d539-4675-4c74-ec90-7501e1e32d9c"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace the line below with the correct initialization for conv.\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=4,\n",
    "    out_channels=6,\n",
    "    kernel_size=1\n",
    ")\n",
    "\n",
    "x = torch.rand([1, 4, 100, 100])\n",
    "y = conv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ivP-AT4eGzB"
   },
   "source": [
    "### 2b: kernel size\n",
    "Define a convolution kernel with a kernel size of 7x9 and no padding. What will the output shape be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "hu-Tnv_aeGHj",
     "outputId": "e9f25139-0dc3-461f-fdb0-6c8ff46f56cf"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "hu-Tnv_aeGHj",
     "outputId": "cbee0d14-ab9d-4c78-9ea5-433c52dbcaa6"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace the line below with the correct initialization for conv.\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=16,\n",
    "    out_channels=32,\n",
    "    kernel_size=(7,9)\n",
    ")\n",
    "\n",
    "x = torch.rand([3, 16, 255, 255])\n",
    "y = conv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_tax8OKeYn9"
   },
   "source": [
    "### 2c: padding\n",
    "Define a convolution kernel with size 3x3 and 1 pixel of padding.  What will the ouptut shape be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "GolkUgaVekMC",
     "outputId": "390a6502-f1fd-426a-f970-a9515070afeb"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "GolkUgaVekMC",
     "outputId": "8809551a-32f0-4484-fbed-8504e04698ea"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace the line below with the correct initialization for conv.\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=64,\n",
    "    out_channels=128,\n",
    "    kernel_size=3,\n",
    "    padding=1\n",
    ")\n",
    "\n",
    "x = torch.rand([32, 64, 64, 64])\n",
    "y = conv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1YK1j14eswy"
   },
   "source": [
    "### 2d: stride\n",
    "\n",
    "Define a convolutional kernel with size 5x5 and stride 2.  What will the output shape be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "OJum221Ye7qU",
     "outputId": "cc9113f3-957f-4d0f-979c-43d3f60beee0"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "OJum221Ye7qU",
     "outputId": "14fbb604-1599-4d19-ba04-61b6fa07fffd"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace the line below with the correct initialization for conv.\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=64,\n",
    "    out_channels=128,\n",
    "    kernel_size=5,\n",
    "    padding=2,\n",
    "    stride=2\n",
    ")\n",
    "\n",
    "x = torch.rand([32, 64, 64, 64])\n",
    "y = conv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiFpaoXDe942"
   },
   "source": [
    "### 2e: dilation\n",
    "Define a convolutional kernel with size 7x7 and stride 1, and dilation 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "9mM0KfzJe9Z4",
     "outputId": "af471eb4-721d-4e28-b235-776a7d827625"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "9mM0KfzJe9Z4",
     "outputId": "90788115-3a9c-4770-e2de-74e2f68185cf"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace the line below with the correct initialization for conv.\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=64,\n",
    "    out_channels=32,\n",
    "    kernel_size=7,\n",
    "    stride=1,\n",
    "    dilation=2\n",
    ")\n",
    "\n",
    "x = torch.rand([32, 64, 64, 64])\n",
    "y = conv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jwsESxPfalJ"
   },
   "source": [
    "## Exercise 3: Image classification\n",
    "Now we will try to classify pictures of numbers by learning filters with a CNN.  We will need to import torchvision to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 178
     },
     "id": "m0Y5AHudCsFS",
     "outputId": "20b2d696-96b4-4cb8-ef4b-1096537076ce"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 177
     },
     "id": "m0Y5AHudCsFS",
     "outputId": "dd29119e-55e0-42c7-8de1-785e08d1ade7"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Let's go ahead and seed the random number generator so that we can reproduce our results\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "mnist = torchvision.datasets.MNIST(\n",
    "    root='sample_data',\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "print(f'Total No. Images: {len(mnist)}')\n",
    "\n",
    "print('Sample of images with ground truth class data:')\n",
    "fig, ax = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "  ax[i].imshow(mnist[i][0].squeeze())\n",
    "  ax[i].set_xlabel(mnist[i][1])\n",
    "  ax[i].get_xaxis().set_ticks([])\n",
    "  ax[i].get_yaxis().set_ticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1wWObDjM0nl"
   },
   "source": [
    "### Exercise 3a: Splitting the Dataset\n",
    "Okay, we've downloaded our dataset and looked at some images.  Now let's split our dataset.  Can you create an 80-10-10 Train/Val/Test split?*italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "Mw4AXzYzGyPx",
     "outputId": "07516fc4-0071-45b5-d255-6b8f7e142b84"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "Mw4AXzYzGyPx",
     "outputId": "bc5b70f0-eb4a-419e-b138-c3f8fb44afb5"
    }
   },
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "    mnist,\n",
    "    [int(0.8 * len(mnist)), int(0.1 * len(mnist)), int(0.1 * len(mnist))]\n",
    ")\n",
    "print(f'No. Train Images: {len(train_set)}')\n",
    "print(f'No. Val Images: {len(val_set)}')\n",
    "print(f'No. Test Images: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LTl7hxdNvlF"
   },
   "source": [
    "### Exercise 3b: LeNet\n",
    "A neural network for LeNet is coded below, but it's missing its first convolution in the ```forward()``` method.  Can you complete it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22DbhSvheAeb"
   },
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # First Convolution\n",
    "    self.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=4,\n",
    "        kernel_size=5,\n",
    "        stride=1,\n",
    "        padding=0\n",
    "    )\n",
    "    self.batch_norm1 = torch.nn.BatchNorm2d(4)\n",
    "    self.activation1 = torch.nn.ReLU()\n",
    "    self.pool1 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "    # Second Convolution\n",
    "    self.conv2 = torch.nn.Conv2d(\n",
    "        in_channels=4,\n",
    "        out_channels=12,\n",
    "        kernel_size=5,\n",
    "        stride=1,\n",
    "        padding=0\n",
    "    )\n",
    "    self.batch_norm2 = torch.nn.BatchNorm2d(12)\n",
    "    self.activation2 = torch.nn.ReLU()\n",
    "    self.pool2 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    self.fully_connected1 = torch.nn.Linear(\n",
    "        in_features=4 * 4 * 12,\n",
    "        out_features=10\n",
    "    )\n",
    "    self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # First Convolution\n",
    "    x = self.conv1(x)\n",
    "    x = self.batch_norm1(x)\n",
    "    x = self.activation1(x)\n",
    "    x = self.pool1(x)\n",
    "\n",
    "    # Second Convolution\n",
    "    x = self.conv2(x)\n",
    "    x = self.batch_norm2(x)\n",
    "    x = self.activation2(x)\n",
    "    x = self.pool2(x)\n",
    "\n",
    "    # Head\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fully_connected1(x)\n",
    "    y_hat = self.softmax(x)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYUdtLWiOaLv"
   },
   "source": [
    "### Excercise 3c: Training\n",
    "Now we need to train our model.  First we will create data loaders, which take data from the dataset and feed it to our model one batch at a time.  We loop over the entire dataset for 10 epochs.  However, there's a problem: the code below isn't calculating our validation loss.  Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "nxvsG0XbVtgb",
     "outputId": "829942d9-04b7-4691-bd88-cbe18aee1b01"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "nxvsG0XbVtgb",
     "outputId": "e5b120a5-031a-459e-ba4d-cf36b15fc520"
    }
   },
   "outputs": [],
   "source": [
    "# If we have a GPU, then use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Create an instance of the model and put it on the GPU\n",
    "model = LeNet()\n",
    "model = model.to(device)\n",
    "\n",
    "# Create 2 dataloaders, one for training and one for validation\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=200,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=200,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# We will use negative log likelihood loss\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "# Let's use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(10):\n",
    "  train_loss.append(0)\n",
    "  val_loss.append(0)\n",
    "  # Training data\n",
    "  for data in train_loader:\n",
    "    # Get data from the loader and give it to the network\n",
    "    x, y = data\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_hat = model(x)\n",
    "\n",
    "    # Calculate loss\n",
    "    L = loss_function(y_hat, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record our result\n",
    "    train_loss[-1] += L\n",
    "\n",
    "  # Cross validation\n",
    "  for data in val_loader:\n",
    "    x, y = data\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "      y_hat = model(x)\n",
    "      L = loss_function(y_hat, y)\n",
    "      val_loss[-1] += L\n",
    "\n",
    "  print(f'Epoch: {epoch} --- Training Loss: {train_loss[-1]:.2f} --- Val Loss: {val_loss[-1]:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu8RIg7QRHhu"
   },
   "source": [
    "### Exercise 3d: Test the model\n",
    "Can you calculate the total number of correct predictions your model makes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "DXFWjWbhyhKX",
     "outputId": "02e8eca6-a455-4bea-cff5-7e9391ee51b9"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "DXFWjWbhyhKX",
     "outputId": "3a8aa4ef-6241-4601-ec1d-78767d19eb27"
    }
   },
   "outputs": [],
   "source": [
    "# Create a test data loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=200\n",
    ")\n",
    "\n",
    "total_predictions = 0\n",
    "total_correct = 0\n",
    "for data in train_loader:\n",
    "  x, y = data\n",
    "  x = x.to(device)\n",
    "  y = y.to(device)\n",
    "  with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "    total_correct += torch.sum(torch.argmax(y_hat, 1) == y)\n",
    "    total_predictions += y_hat.shape[0]\n",
    "\n",
    "print(f'Accuracy: {100 * total_correct/total_predictions:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM8uyv2QRt7R"
   },
   "source": [
    "Now let's visualize our predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 142
     },
     "id": "nGMyUOUk1DzZ",
     "outputId": "e6fe1e26-b237-423e-e81f-a531bb0eae1d"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 197
     },
     "id": "nGMyUOUk1DzZ",
     "outputId": "ca18c411-ae81-4d50-ee02-8434859ac73f"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize Predictions\n",
    "fig, ax = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "  ax[i].imshow(mnist[i][0].squeeze())\n",
    "\n",
    "  with torch.no_grad():\n",
    "    y_hat = model(mnist[i][0].unsqueeze(0).to(device))\n",
    "    ax[i].set_xlabel(f'GT: {mnist[i][1]}, NN: {torch.argmax(y_hat, 1).item()}')\n",
    "\n",
    "  ax[i].get_xaxis().set_ticks([])\n",
    "  ax[i].get_yaxis().set_ticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xWCH01tR-hh"
   },
   "source": [
    "We can also visualize the features we learned.  For more information on explaining what a CNN learned, check out [this blog](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 144
     },
     "id": "cnWwUJMv2cG5",
     "outputId": "557dbeec-1443-4e40-900f-1c5f10cee8df"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 144
     },
     "id": "cnWwUJMv2cG5",
     "outputId": "9bb8bf4c-068d-4ea2-eac1-c585c5aed838"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4)\n",
    "for i in range(4):\n",
    "  filter = model.cpu().conv1.weight[i,0,:,:].squeeze().detach().numpy()\n",
    "  ax[i].imshow(filter)\n",
    "  ax[i].get_xaxis().set_ticks([])\n",
    "  ax[i].get_yaxis().set_ticks([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL5i__nUTPL5"
   },
   "source": [
    "### Exercise 3e: Can you do better?\n",
    "Now see if you can modify the training code or the architecture above to get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYQy5UDsTzVG"
   },
   "source": [
    "## Exercise 4: Loading a Pretrained Model\n",
    "Now we will load a pretrained model (Resnet 18) trained on Imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "khhk86ONT3Ka",
     "outputId": "be6d28eb-7f05-431c-f5b7-ff8855f565f6"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "khhk86ONT3Ka",
     "outputId": "7470750f-38b1-4288-b6e1-36967845d646"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYrlijWMUDTu"
   },
   "source": [
    "First, we will grab an image of a dog and visualize it to know what we're working with.  Here we are using urllib to download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 453
     },
     "id": "sBPFe1xScnvr",
     "outputId": "c7d4c375-f55c-428b-c310-759a6b4f715c"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 452
     },
     "id": "sBPFe1xScnvr",
     "outputId": "46b69419-848c-4107-9c03-38937e94a063"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "urllib.request.urlretrieve(\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "\n",
    "img = Image.open(\"dog.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifmVhYogUciF"
   },
   "source": [
    "### Exercise 4a: Inferencing a Pretrained Model\n",
    "Can you complete the code to classify the image of the dog using Resnet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "OXmW9l25jIY6",
     "outputId": "204e1ce2-ab3a-43ad-92a9-2cd092ab4d2a"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "OXmW9l25jIY6",
     "outputId": "27650b27-748d-4ae4-b242-eb44c7ced05c"
    }
   },
   "outputs": [],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "x = preprocess(img)\n",
    "\n",
    "y = model(x.unsqueeze(0))\n",
    "probabilities = torch.nn.functional.softmax(y[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m08nUYVVCMN"
   },
   "source": [
    "Now let's interpret those results.  Each dimension in the output array corresponds to one class, but Imagenet has 1000 classes.  Luckily there is a list that maps Imagenet dimension number to classes.  Let's use it to find out what we actually predicted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "olvrvsNPkaQS",
     "outputId": "396786d3-da57-4e9e-d599-fec71444e02b"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "olvrvsNPkaQS",
     "outputId": "4dd8b6c1-59ef-44b0-d15a-f3f2aa08642d"
    }
   },
   "outputs": [],
   "source": [
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "\n",
    "# Load the file as a list in python\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Show the top 5 most likely 5 categories\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JH1gh3WVhn3"
   },
   "source": [
    "Now we're going to grab a dataset of different Pokemon.  This dataset comes in a zip format, so we'll use some new libraries to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbMfQQDUlnXu"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://osf.io/u4njm/download', stream=True)\n",
    "archive = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "archive.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIX8Q8RYYuNC"
   },
   "source": [
    "### Exercise 4b: Fine-tuning Dataset\n",
    "Before we can perform transfer learning on the Pokemon dataset, we need to split between training, validation, and test.  Can you complete the code below to make that happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "id": "kC8kxrtKV8_4"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "kC8kxrtKV8_4",
     "outputId": "9e208824-89fe-44ee-b76c-b768db3ef5a8"
    }
   },
   "outputs": [],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "  torchvision.transforms.Resize((224, 224)),\n",
    "  torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "pokemon = torchvision.datasets.ImageFolder(\n",
    "  'small_pokemon_dataset',\n",
    "  transform=preprocess\n",
    ")\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "  pokemon,\n",
    "  [int(0.8 * len(pokemon)), int(0.1 * len(pokemon)), len(pokemon) - int(0.8 * len(pokemon)) - int(0.1 * len(pokemon))]\n",
    ")\n",
    "print(f'No. Train Images: {len(train_set)}')\n",
    "print(f'No. Val Images: {len(val_set)}')\n",
    "print(f'No. Test Images: {len(test_set)}')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=16\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrpJkeRMZY2L"
   },
   "source": [
    "### Excercise 4c: Modifying our pretrained model\n",
    "Resent comes with 1000 output neurons, but we only have 9 classes.  Can you replace the last layer of the model to only have 9 outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4jLpjb1ZOQh"
   },
   "outputs": [],
   "source": [
    "no_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(no_features, 9)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWaX0Um9Zz4K"
   },
   "source": [
    "### Excercise 4d: Training Loop\n",
    "Can you complete the code below to use cross-entropy loss during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "x_IOmZtGmgPQ",
     "outputId": "eba8c48b-d5a5-4db4-ff01-a97bbdb420b3"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "x_IOmZtGmgPQ",
     "outputId": "d009adbe-f698-462a-be13-8cd883e7dee6"
    }
   },
   "outputs": [],
   "source": [
    "# Set up our optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Allow model gradients to be updated\n",
    "model.train()\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(10):\n",
    "  train_loss.append(0)\n",
    "  val_loss.append(0)\n",
    "\n",
    "  # Train loop\n",
    "  for batch in train_loader:\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    y_hat = model(x)\n",
    "    L = loss_function(y_hat, y)\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss[-1] += L\n",
    "\n",
    "  # Validation loop\n",
    "  for data in val_loader:\n",
    "    x, y = data\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "      y_hat = model(x)\n",
    "      L = loss_function(y_hat, y)\n",
    "      val_loss[-1] += L\n",
    "\n",
    "  print(f'Epoch: {epoch} --- Training Loss: {train_loss[-1]:.2f} --- Val Loss: {val_loss[-1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDaRx4u-uD_A"
   },
   "source": [
    "### Exercise 4e: Prediction\n",
    "Let's take an image from our dataset.  How did we do at prediction.  Can you complete the code below to find out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "nIQ8HeogudJU",
     "outputId": "5b9dbe41-d844-4de6-ed98-923e6fe394a5"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "nIQ8HeogudJU",
     "outputId": "31477ca3-b330-43b3-b9fa-dc9164cb516b"
    }
   },
   "outputs": [],
   "source": [
    "img = Image.open('small_pokemon_dataset/Blastoise/03bc036b642d4873985399cebfd0bb64.jpg')\n",
    "x = preprocess(img).to(device)\n",
    "\n",
    "y = model(x.unsqueeze(0))\n",
    "probabilities = torch.nn.functional.softmax(y[0], dim=0)\n",
    "for name, idx in pokemon.class_to_idx.items():\n",
    "  print(f'{name}: {100 * probabilities[idx]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_1uVcWovNaf"
   },
   "source": [
    "## Exercise 5: Segmentation\n",
    "Now we will use UNET, an early model for image segmentation to look at identifying brain tumors in CT scan images.  First we will load the model from torch hub and download an image to test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "id": "6vKirmaSaoxR"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 452
     },
     "id": "6vKirmaSaoxR",
     "outputId": "7cb2b354-9d3e-4b73-f123-8995e6e595a5"
    }
   },
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
    "\n",
    "# Load Test Image\n",
    "urllib.request.urlretrieve(\"https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png\", \"TCGA_CS_4944.png\")\n",
    "img = Image.open(\"TCGA_CS_4944.png\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahKrWKEwbTwD"
   },
   "source": [
    "The code to preprocess the image is provided for you below, can you preprocess the image and inference the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 871
     },
     "id": "X_ADDlGYvReS",
     "outputId": "937d2555-ca09-4f0b-efa0-7ed549fdc2f9"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "X_ADDlGYvReS",
     "outputId": "06b50816-4e21-4a1c-c75a-1dfdc3a025b4"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "mu, sig = numpy.mean(img, axis=(0, 1)), numpy.std(img, axis=(0, 1))\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=mu, std=sig),\n",
    "])\n",
    "x = preprocess(img)\n",
    "\n",
    "y = model(x.unsqueeze(0))\n",
    "\n",
    "plt.imshow(y.squeeze(0,1).detach().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
