{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Lo51io6pAgos",
   "metadata": {
    "id": "Lo51io6pAgos"
   },
   "source": [
    "# Deep Learning with Images Bootcamp Session 3\n",
    "<a href=\"https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2026/blob/main/SESSION3/session3_part2_student.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "In the second half of the session, we will explore three key computer vision problems:\n",
    "\n",
    "  - **Image Classification** (LeNet)\n",
    "  - **Segmentation** (U-Net)\n",
    "  - **Diffusion Models** (DDPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Student Notebook Instructions (Read Me)\n",
    "\n",
    "This notebook is **interactive**. Most cells can be run as-is, but some cells include **TODO** sections where you will type a few lines of code.\n",
    "\n",
    "**How to use this notebook**\n",
    "- Run cells top to bottom.\n",
    "- When you see a **TODO**, pause and complete it.\n",
    "- After each TODO, answer the short **Student Task** prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2khRITZBBWZ",
   "metadata": {
    "id": "P2khRITZBBWZ"
   },
   "source": [
    "# Exercise 1 - Image Classification\n",
    "\n",
    "**Overall Pipeline:**\n",
    "\n",
    "1. Data Collection and Annotation\n",
    "2. Data Pre-processing\n",
    "3. Model Development\n",
    "4. Model Training\n",
    "5. Fine-tuning\n",
    "6. Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5F3zk86aDi8P",
   "metadata": {
    "id": "5F3zk86aDi8P"
   },
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FcBD__3rDnBs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcBD__3rDnBs",
    "outputId": "9a95b80c-0509-4d40-920b-c4e67ae914e2"
   },
   "outputs": [],
   "source": [
    "# ---- Import ----\n",
    "import os, random, math, time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kF5VoLelAljP",
   "metadata": {
    "id": "kF5VoLelAljP"
   },
   "source": [
    "### Downloading the dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LFh6iQtqAhkS",
   "metadata": {
    "id": "LFh6iQtqAhkS"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "\n",
    "kaggle_json = {\n",
    "    \"username\": \"Sankeerthana03\",\n",
    "    \"key\": \"KGAT_f31deeeaecd714f2bfd3b656e5237ffe\"\n",
    "}\n",
    "\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "    json.dump(kaggle_json, f)\n",
    "\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xKqFIL-ODO0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKqFIL-ODO0e",
    "outputId": "5430b27e-aa62-432a-f4b0-61c58c5d58d3"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets list | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xs6Gey7yAhbN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Xs6Gey7yAhbN",
    "outputId": "94f46b66-dcb0-49c5-e1ae-c4e3bf774ba3"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d youssifhisham/colored-mnist-dataset\n",
    "!unzip colored-mnist-dataset.zip -d colorized_mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rixBmNoRDwyb",
   "metadata": {
    "id": "rixBmNoRDwyb"
   },
   "source": [
    "## 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y8jvJ2O6Dzbr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8jvJ2O6Dzbr",
    "outputId": "b47f47f6-b056-4810-e368-ebdd147161c5"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = r\"/content/colorized_mnist/colorized-MNIST-master\"  # change this\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"training\")\n",
    "TEST_DIR  = os.path.join(DATA_DIR, \"testing\")\n",
    "\n",
    "# LeNet expects small inputs; we'll resize to 32x32 (classic LeNet setting)\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std  = (0.5, 0.5, 0.5)\n",
    "\n",
    "# NOTE: TO ensure that the analysis of the model's performance is fair, we keep the transforms and augmentations the same for\n",
    "# both training and testing sets.\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Scans the data directory for class-named subfolders and pairs the images with their labels and applies the transforms\n",
    "full_train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "test_ds = datasets.ImageFolder(TEST_DIR,  transform=test_tfms)\n",
    "# output - (image, label)\n",
    "\n",
    "# Extracting the class_names out to allow easier analysis\n",
    "class_names = full_train_ds.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6MTJo50rD-io",
   "metadata": {
    "id": "6MTJo50rD-io"
   },
   "source": [
    "### Displaying the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MfP4md-SEAe1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "id": "MfP4md-SEAe1",
    "outputId": "679e30fc-13a2-495b-d157-364f1803866d"
   },
   "outputs": [],
   "source": [
    "def show_batch(ds, n=16):\n",
    "    idxs = np.random.choice(len(ds), n, replace=False)\n",
    "    imgs, labels = zip(*[ds[i] for i in idxs])\n",
    "\n",
    "    # unnormalize for display\n",
    "    imgs = torch.stack(imgs)\n",
    "    imgs = imgs * torch.tensor(std).view(1,3,1,1) + torch.tensor(mean).view(1,3,1,1)\n",
    "    imgs = imgs.clamp(0,1)\n",
    "\n",
    "    cols = int(math.sqrt(n))\n",
    "    rows = math.ceil(n / cols)\n",
    "    plt.figure(figsize=(cols*2.2, rows*2.2))\n",
    "    for i in range(n):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(imgs[i].permute(1,2,0))\n",
    "        plt.title(str(labels[i]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_batch(full_train_ds, n=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F9pEbadRIsF9",
   "metadata": {
    "id": "F9pEbadRIsF9"
   },
   "source": [
    "###  Splitting the Dataset\n",
    "\n",
    "General Practice is to split the dataset into 3 splits - Training, Validation and Test. What is usually given in the dataset is a training and a test set, where training set contains the image-label pairs that the model learns from, and the test set is a subset of the dataset that the model has never seen. The performance of the model is analysed on how it performs on the test set (unseen data) to replicate real-world scenarios. However, it is highly encouraged (almost all the time) to have an intermediary 'validation' set to act as a 'pseudo test set' during the training process, so the model gets feedback (self-check) on how it's learning process and rectifies its learning.\n",
    "\n",
    "- Training Set - what the model sees and learns from\n",
    "- Validation Set - pseudo-test set for the model to use as its playing ground to rectify it's learning during training\n",
    "- Test Set - unseen data for the model\n",
    "\n",
    "NOTE: The integrity of the experiment/model's learning is compromised is there is any form of data leakage across these sets.\n",
    "\n",
    "The whole dataset is usually divided into a ratio of 80-10-10 or 70-20-10 (Training-Validation-Test) sets. However, in this case, the dataset in Kaggle is already split, we'll go with the given split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58tcpkwhIpAC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58tcpkwhIpAC",
    "outputId": "e7c4093c-78e6-458e-ba16-38c0352240a0"
   },
   "outputs": [],
   "source": [
    "total_train = int(len(full_train_ds))\n",
    "total_test = int(len(test_ds))\n",
    "\n",
    "print(f\"Total number of images in the full training set: {total_train}\")\n",
    "print(f\"Total number of images in the test set: {total_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKJWCwtpIxC4",
   "metadata": {
    "id": "tKJWCwtpIxC4"
   },
   "source": [
    "In this case, to get the validation split, we split the existing training set - (15% of the existing training set) to get the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkzrgBqqI0Uj",
   "metadata": {
    "id": "kkzrgBqqI0Uj"
   },
   "outputs": [],
   "source": [
    "# Creating a configuration class called CFG and creating an instance of it\n",
    "@dataclass\n",
    "class CFG:\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 2\n",
    "    val_frac: float = 0.15\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# Calculating the number of images of the validation set\n",
    "val_size = int(len(full_train_ds) * cfg.val_frac)\n",
    "train_size = len(full_train_ds) - val_size\n",
    "\n",
    "# Splitting the training set to get the vaidation set\n",
    "train_ds, val_ds = random_split(full_train_ds, [train_size, val_size],\n",
    "                                generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ux-U8ohxI2Wu",
   "metadata": {
    "id": "ux-U8ohxI2Wu"
   },
   "source": [
    "#### DataLoader\n",
    "\n",
    "Dataloader is what feeds the data into the model batch by batch in the right order. It is responsible for arranging data into batches, shuffling and loading data efficiently during training.\n",
    "\n",
    "We usually cannot load the entire dataset into the model at once. Instead, we train using mini-batches, and the DataLoader automates this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Student Task: Data Loaders\n",
    "\n",
    "Using train_loader as an example, code the data loader for test and val as well. \n",
    "\n",
    "**NOTE:** Do **NOT** shuffle the data for validation and test sets.\n",
    "\n",
    "**Question to Think about:**\n",
    "- Why do we not shuffle the validation and test sets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aZ2gLqQ7I4Bw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZ2gLqQ7I4Bw",
    "outputId": "d45755bd-e35b-4404-c534-f0c97a122f1a"
   },
   "outputs": [],
   "source": [
    "# creating the data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader   = \"\"\" To Do \"\"\"\n",
    "test_loader  = \"\"\" TO Do \"\"\"\n",
    "\n",
    "print(f\"Total Number of images in each set after the split:\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"Training Set: {len(train_ds)}\")\n",
    "print(f\"Validation Set: {len(val_ds)}\")\n",
    "print(f\"Test Set: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rMlcm6hiI7B7",
   "metadata": {
    "id": "rMlcm6hiI7B7"
   },
   "source": [
    "### Model Building\n",
    "\n",
    "Building LeNet from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HEW0pJERI9Ft",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEW0pJERI9Ft",
    "outputId": "a9e15467-1572-470b-8717-7aa8ea28dca8"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),  # 32->28\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),                # 28->14\n",
    "\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0), # 14->10\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),                # 10->5\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                 # 16*5*5 = 400\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet(num_classes=10).to(device)\n",
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_ws5hN1JFWV",
   "metadata": {
    "id": "H_ws5hN1JFWV"
   },
   "source": [
    "#### Minibatches\n",
    "\n",
    "\n",
    "**What is a Mini-Batch?**\n",
    "\n",
    "Assume we have **10 images** in our dataset:\n",
    "\n",
    "> Images = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
    "\n",
    "We choose a **batch size = 4**\n",
    "\n",
    "The dataset is grouped into small chunks (mini-batches):\n",
    "\n",
    "> Mini-batch 1 = {1, 2, 3, 4}\n",
    "\n",
    "> Mini-batch 2 = {5, 6, 7, 8}\n",
    "\n",
    "> Mini-batch 3 = {9, 10}\n",
    "\n",
    "\n",
    "- Each mini-batch is processed **one at a time**\n",
    "- Each mini-batch produces **one model update**\n",
    "\n",
    "\n",
    "**How Many Mini-Batches Are There in One Epoch?**\n",
    "\n",
    "We use simple division, so:\n",
    "\n",
    "Total samples = 10\n",
    "Batch size = 4\n",
    "\n",
    "Number of mini-batches = \\frac{10}{4} = 2.5\n",
    "\n",
    "Since we cannot have half a mini-batch, we round up, giving:\n",
    "\n",
    "Number of mini-batches = 3\n",
    "\n",
    "So:\n",
    "\n",
    "1 epoch = 3 mini-batches\n",
    "\n",
    "> **1 epoch = 3 mini-batches**\n",
    "\n",
    "**What the Model Actually Sees**\n",
    "\n",
    "Instead of learning from one image at a time, the model learns from a group of images. This makes training:\n",
    "- Faster\n",
    "- More stable\n",
    "- Memory-efficient\n",
    "\n",
    "\n",
    "**What Happens When We Shuffle the Data?**\n",
    "\n",
    "Before forming mini-batches, the data order is randomly shuffled.\n",
    "\n",
    "> Original order: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
    "\n",
    "> Shuffled order: {6, 2, 9, 1, 8, 3, 10, 4, 7, 5}\n",
    "\n",
    "Mini-batches are then created from the shuffled data:\n",
    "\n",
    "> Mini-batch 1 = {6, 2, 9, 1}\n",
    "\n",
    "> Mini-batch 2 = {8, 3, 10, 4}\n",
    "\n",
    "> Mini-batch 3 = {7, 5}\n",
    "\n",
    "Shuffling helps the model generalize better and prevents it from learning patterns based on data order.\n",
    "\n",
    "**Key Takeaway:**\n",
    "A mini-batch is simply a small group of samples that the model learns from at one time, instead of using the entire dataset at once.\n",
    "\n",
    "**How This Looks in PyTorch:**\n",
    "\n",
    "```python\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yUQXySFfJI6d",
   "metadata": {
    "id": "yUQXySFfJI6d"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V8tpH-A5JOJF",
   "metadata": {
    "id": "V8tpH-A5JOJF"
   },
   "source": [
    "### Training Loop and Storing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5E6vLFjJQWw",
   "metadata": {
    "id": "y5E6vLFjJQWw"
   },
   "outputs": [],
   "source": [
    "def accuracy_from_logits(logits, y):\n",
    "    \"\"\"\n",
    "    Function that returns the model's prediction - class index with the highest score\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(dim=1)  #argmax picks the class index with the highest score for each image\n",
    "\n",
    "    # preds==y --> gives [True, False...True] - a boolean array\n",
    "    # Converting it into float() --> gives [1,0,...1] and then taking mean of it\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    This functions entails the computations happening in one training epoch\n",
    "    which includes, generating the logits -> calculating the loss ->\n",
    "    backpropagation -> updating parameters\n",
    "    \"\"\"\n",
    "    # puts the model in training mode\n",
    "    model.train()\n",
    "    running_loss, running_acc = 0.0, 0.0 # initializing loss and accuracy\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)  # moving x,y to gpu/cpu so that computations happen on the same device\n",
    "\n",
    "        # Clearing old gradients. Pytorch accumulates gradient by default, so\n",
    "        # if you dont clear them, gradients become incorrect.\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass: model produces raw class scores\n",
    "        logits = model(x)\n",
    "\n",
    "        # computes how wrong the model is\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the loss and accuracy across batches and return the average\n",
    "        running_loss += loss.item()\n",
    "        running_acc  += accuracy_from_logits(logits, y)\n",
    "\n",
    "    return running_loss / len(loader), running_acc / len(loader)\n",
    "\n",
    "@torch.no_grad() # --> disables gradient tracking\n",
    "def eval_one_epoch(model, loader, criterion):\n",
    "    \"\"\"\n",
    "    Function to code for validation in one epoch.\n",
    "    \"\"\"\n",
    "    # puts the model in evaluating mode\n",
    "    model.eval()\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # getting the logits\n",
    "        logits = model(x)\n",
    "\n",
    "        # computing how wrong the model is\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # updating loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        running_acc  += accuracy_from_logits(logits, y)\n",
    "\n",
    "    return running_loss / len(loader), running_acc / len(loader)\n",
    "\n",
    "def fit(model, train_loader, val_loader, lr=1e-3, weight_decay=0.0, epochs=10):\n",
    "    # Loss Instantiation\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instantiating the optimizer with the parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Dictionary to keep track of Training progress - useful for analysis later\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "\n",
    "    best_val_acc = -1\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        va_loss, va_acc = eval_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "        # This saves the model weights when validation accuracy improves\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "              f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "              f\"val loss {va_loss:.4f} acc {va_acc:.4f} | {dt:.1f}s\")\n",
    "\n",
    "    return history, best_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4R2JwBzJ1LxI",
   "metadata": {
    "id": "4R2JwBzJ1LxI"
   },
   "source": [
    "In image classification, our model takes an image x and outputs logits — a score for each class.\n",
    "\n",
    "Training means:\n",
    "- Forward pass: logits = model(x)\n",
    "\n",
    "- Compute loss: compare logits vs true label y.\n",
    "\n",
    "- Backpropagation: compute gradients\n",
    "\n",
    "- Update weights: optimizer steps in the direction that reduces loss\n",
    "Repeat for many batches and many epochs\n",
    "\n",
    "We also run a validation loop to check generalization (no learning, just evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke67nhhEJTGR",
   "metadata": {
    "id": "ke67nhhEJTGR"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "We start the training process of the model and monitor it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-0yTKRnkJVh_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0yTKRnkJVh_",
    "outputId": "7fcfee85-cf97-4d91-ede9-c49b144495f4"
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = LeNet(num_classes=10).to(device)\n",
    "\n",
    "history, best_state = fit(\n",
    "    model, train_loader, val_loader,\n",
    "    lr=1e-3, weight_decay=0.0, epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y0MCCRDPJXWJ",
   "metadata": {
    "id": "y0MCCRDPJXWJ"
   },
   "source": [
    "## Performance Analysis\n",
    "\n",
    "We analyse the training and validation loss and accuracy curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8D4YOGERJbE8",
   "metadata": {
    "id": "8D4YOGERJbE8"
   },
   "source": [
    "### Plotting the Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Student Task: Interpreting Curves\n",
    "Overfitting: train improves but val plateaus/worsens (gap grows). \n",
    "\n",
    "Underfitting: both stay poor. \n",
    "\n",
    "Write 2 lines about the curves plotted below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_9D_vxo3JdS3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "_9D_vxo3JdS3",
    "outputId": "3863e3ec-b8d8-4e59-9189-78e6e8cdc629"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    epochs = range(1, len(history[\"train_loss\"])+1)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"train loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"val loss\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(\"Loss curves\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"train acc\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"val acc\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.title(\"Accuracy curves\")\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qg0jDXNgpAyP",
   "metadata": {
    "id": "Qg0jDXNgpAyP"
   },
   "source": [
    "**What is “loss”?**\n",
    "\n",
    "**What we observe:**\n",
    "\n",
    "\n",
    "**Key takeaway:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xwR6wuqkpW-i",
   "metadata": {
    "id": "xwR6wuqkpW-i"
   },
   "source": [
    "**What is “accuracy”?**\n",
    "\n",
    "\n",
    "**What we observe:**\n",
    "\n",
    "**Key takeaway:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SODyRtSgJkE-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SODyRtSgJkE-",
    "outputId": "8c5a7175-2d64-4a7e-e18a-70bc15e9b6f6"
   },
   "outputs": [],
   "source": [
    "# reverting to the model's best performance for evaluation purposes\n",
    "# load best on val\n",
    "model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTAH-WhWJpG1",
   "metadata": {
    "id": "kTAH-WhWJpG1"
   },
   "source": [
    "### Testing the Model on Unseen Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836aff2",
   "metadata": {},
   "source": [
    "### ✅ Student Task: Understanding Importance of Unseen Data\n",
    "\n",
    "**Question to think:**\n",
    " - Why is it important to evaluate the model on unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FcQf0uRVJr7D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcQf0uRVJr7D",
    "outputId": "ee07c302-f968-44d2-fe4c-83af6ccc1e4a"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_acc = eval_one_epoch(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eKabIBkhsAy4",
   "metadata": {
    "id": "eKabIBkhsAy4"
   },
   "source": [
    "This is in line with the loss and accuracy curves shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sMlHhlZSJw0l",
   "metadata": {
    "id": "sMlHhlZSJw0l"
   },
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "Keeping the model architecture fixed, but carefully adjusting the training hyperparameters so the model learns better, faster, and more stably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7BiZifuXtaiQ",
   "metadata": {
    "id": "7BiZifuXtaiQ"
   },
   "source": [
    "### Random Search\n",
    "\n",
    "Random search is a hyperparameter optimization technique that randomly selects and tests combinations of hyperparameters from defined distributions, rather than checking every possible combination like grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ImjSFgWTJzwF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImjSFgWTJzwF",
    "outputId": "e0c226d3-5b55-49b6-a735-da5bb00c8790"
   },
   "outputs": [],
   "source": [
    "def sample_params():\n",
    "    # log-uniform for LR is common\n",
    "    lr = 10 ** np.random.uniform(-4, -2)        # 1e-4 to 1e-2\n",
    "    wd = 10 ** np.random.uniform(-6, -3)        # 1e-6 to 1e-3\n",
    "    batch_size = int(np.random.choice([64, 128, 256]))\n",
    "    return {\"lr\": lr, \"weight_decay\": wd, \"batch_size\": batch_size}\n",
    "\n",
    "def run_trial(params, epochs=6, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    # rebuild loaders if batch size changes\n",
    "    train_loader = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=params[\"batch_size\"], shuffle=False,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "    model = LeNet(num_classes=10).to(device)\n",
    "    history, best_state = fit(model, train_loader, val_loader,\n",
    "                             lr=params[\"lr\"], weight_decay=params[\"weight_decay\"],\n",
    "                             epochs=epochs)\n",
    "    best_val_acc = max(history[\"val_acc\"])\n",
    "    return best_val_acc, history\n",
    "\n",
    "def random_search(n_trials=6, epochs=6):\n",
    "    results = []\n",
    "    for t in range(n_trials):\n",
    "        params = sample_params()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Trial {t+1}/{n_trials} params:\", params)\n",
    "        best_val_acc, history = run_trial(params, epochs=epochs, seed=42+t)\n",
    "        results.append((best_val_acc, params, history))\n",
    "        print(f\"Trial {t+1} best val acc: {best_val_acc:.4f}\")\n",
    "    results.sort(key=lambda x: x[0], reverse=True)\n",
    "    return results\n",
    "\n",
    "results = random_search(n_trials=5, epochs=5)\n",
    "best_val_acc, best_params, best_hist = results[0]\n",
    "best_val_acc, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WYRfbl9HJ3Ns",
   "metadata": {
    "id": "WYRfbl9HJ3Ns"
   },
   "source": [
    "### Training the Model with the Best Set of Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10286c5",
   "metadata": {},
   "source": [
    "### ✅ Student Task: Hyperparameter Tuning\n",
    "\n",
    "**Question to Answer:**\n",
    "- Why choose best params by **validation** accuracy (not training)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TjRIqGJqJ6Bo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjRIqGJqJ6Bo",
    "outputId": "152ef08a-6343-4479-a52c-c55cbbbf0c77"
   },
   "outputs": [],
   "source": [
    "set_seed(123)\n",
    "final_model = LeNet(num_classes=10).to(device)\n",
    "\n",
    "final_train_loader = DataLoader(train_ds, batch_size=best_params[\"batch_size\"], shuffle=True,\n",
    "                                num_workers=cfg.num_workers, pin_memory=True)\n",
    "final_val_loader   = DataLoader(val_ds, batch_size=best_params[\"batch_size\"], shuffle=False,\n",
    "                                num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "final_hist, best_state = fit(final_model, final_train_loader, final_val_loader,\n",
    "                             lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"],\n",
    "                             epochs=12)\n",
    "\n",
    "# loading the best performance of the model\n",
    "final_model.load_state_dict(best_state)\n",
    "\n",
    "# Evaluating it on the test set\n",
    "test_loss, test_acc = eval_one_epoch(final_model, test_loader, nn.CrossEntropyLoss())\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QQY-f1KHJ99-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "QQY-f1KHJ99-",
    "outputId": "53fa3bde-88d9-4e9e-aed5-d51df2b1672f"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534i0mYHuXp9",
   "metadata": {
    "id": "534i0mYHuXp9"
   },
   "source": [
    "**What we see:**\n",
    "- Training loss decreases rapidly in the first few epochs.\n",
    "\n",
    "- Validation loss also decreases steadily and closely follows training loss.\n",
    "\n",
    "- Both curves flatten smoothly after ~epoch 6–7.\n",
    "\n",
    "**What this means:**\n",
    "- The model is learning effectively\n",
    "\n",
    "  - The sharp drop in loss early on indicates the model is quickly learning useful features (edges, strokes, shapes).\n",
    "\n",
    "- Good generalization\n",
    "  - Training and validation losses stay very close.\n",
    "  - There is no divergence where validation loss increases while training loss decreases.\n",
    "\n",
    "- Well-chosen hyperparameters\n",
    "  - The learning rate is not too high (no instability or oscillation).\n",
    "  - The learning rate is not too low (training progresses efficiently).\n",
    "  - Weight decay is helping regularize without hurting learning.\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "When both training and validation loss decrease together and stabilize, it indicates stable learning and good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5lFPVyEZKMXE",
   "metadata": {
    "id": "5lFPVyEZKMXE"
   },
   "source": [
    "# Exercise 2 - Image Segmentation\n",
    "\n",
    "Image segmentation is a computer vision technique that partitions a digital image into distinct, meaningful regions or segments at the pixel level.\n",
    "\n",
    "\n",
    "<table align=\"center\">\n",
    "    <tr>\n",
    "        <td align=\"center\">\n",
    "            <img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/TCGA_DU_6404_19850629.gif\"\n",
    "                 alt=\"TCGA_DU_6404_19850629\" width=\"250\">\n",
    "        </td>\n",
    "        <td align=\"center\">\n",
    "            <img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/TCGA_HT_7879_19981009.gif\"\n",
    "                 alt=\"TCGA_HT_7879_19981009\" width=\"250\">\n",
    "        </td>\n",
    "        <td align=\"center\">\n",
    "            <img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/TCGA_CS_4944_20010208.gif\"\n",
    "                 alt=\"TCGA_CS_4944_20010208\" width=\"250\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"center\"><b>94% DSC</b></td>\n",
    "        <td align=\"center\"><b>91% DSC</b></td>\n",
    "        <td align=\"center\"><b>89% DSC</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Now we will use **U-Net**, an early model for image segmentation to look at identifying brain tumors in CT scan images.  First we will load the model from torch hub and download an image to test on.\n",
    "\n",
    "<figure>\n",
    "<p style=\"text-align:center;\"  align = \"center\"><img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/unet.png\" alt=\"Trulli\" style=\"width:100%\"  align = \"center\"></p>\n",
    "<figcaption align = \"center\">U-Net Architecture for Brain Segmentation<a href=\"https://github.com/mateuszbuda/brain-segmentation-pytorch\"> Official GitHub Release</a> </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RKCrICvtKUy7",
   "metadata": {
    "id": "RKCrICvtKUy7"
   },
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4XG0FhAKYvy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4XG0FhAKYvy",
    "outputId": "3e3203d3-2428-428c-b442-8762f39a0d4e"
   },
   "outputs": [],
   "source": [
    "! pip install medpy\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import color\n",
    "from skimage.io import imsave\n",
    "from medpy.filter.binary import largest_connected_component\n",
    "from scipy.ndimage import binary_dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXgBKGD-KbrQ",
   "metadata": {
    "id": "xXgBKGD-KbrQ"
   },
   "source": [
    "## 2. Loading the Model and Test Image\n",
    "\n",
    "**What are Pretrained Models?**\n",
    "\n",
    "A pretrained model is a neural network whose weights have already been learned by training on a large dataset before we use it in our own code.\n",
    "\n",
    "In other words:\n",
    "The model has prior experience.\n",
    "\n",
    "This means:\n",
    "- The U-Net architecture is loaded\n",
    "- The weights were trained beforehand on brain MRI images\n",
    "\n",
    "- The model already knows:\n",
    "  - What brain tissue looks like\n",
    "  - How tumor regions differ from healthy tissue\n",
    "  - How to draw accurate segmentation boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RTS-ISseyfDK",
   "metadata": {
    "id": "RTS-ISseyfDK"
   },
   "source": [
    "**Torch Hub**\n",
    "\n",
    "Torch Hub is a repository of pre-trained deep learning models and an API within the PyTorch library that facilitates research reproducibility and model sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u_OW0_h9Kk6Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "u_OW0_h9Kk6Z",
    "outputId": "450e0036-8829-4e7f-e7ab-d61e65af2e65"
   },
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load Test Image\n",
    "urllib.request.urlretrieve(\"https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png\", \"TCGA_CS_4944.png\")\n",
    "img = Image.open(\"TCGA_CS_4944.png\").convert(\"RGB\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kQemx21RMbs4",
   "metadata": {
    "id": "kQemx21RMbs4"
   },
   "source": [
    "Code to preprocess the image is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-1D6pKGGMZFZ",
   "metadata": {
    "id": "-1D6pKGGMZFZ"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1, 0.1, 0.1], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "x = transform(img).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V1bSh9xJvtfL",
   "metadata": {
    "id": "V1bSh9xJvtfL"
   },
   "source": [
    "## Inferencing the model on the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RloDQB_wMnlx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "RloDQB_wMnlx",
    "outputId": "b5fd9c74-7695-4211-a80d-bb76b81c182b"
   },
   "outputs": [],
   "source": [
    "y_pred_np = y.squeeze().detach().cpu().numpy()\n",
    "y_pred_np = (y_pred_np > 0.5).astype(np.uint8)  # Thresholding\n",
    "y_pred_np = largest_connected_component(y_pred_np)  # Keep largest connected component\n",
    "\n",
    "# Convert image to numpy array\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Generate Edge Map using Binary Dilation\n",
    "def create_boundary(mask):\n",
    "    edge = binary_dilation(mask) ^ mask  # Create boundary from binary mask\n",
    "    return edge\n",
    "\n",
    "# Generate boundary map\n",
    "boundary_mask = create_boundary(y_pred_np)\n",
    "\n",
    "# Overlay Boundaries on the Image\n",
    "img_with_boundary = img_np.copy()\n",
    "img_with_boundary[boundary_mask == 1] = [255, 0, 0]  # Red outline\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Segmentation Overlay (with Boundaries)\")\n",
    "plt.imshow(img_with_boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qxy5AAeKxZDF",
   "metadata": {
    "id": "Qxy5AAeKxZDF"
   },
   "source": [
    "We can see that the boundaries of the tumour has been located pretty accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfe41f",
   "metadata": {
    "id": "9bcfe41f"
   },
   "source": [
    "# Exercise 3 - Diffusion Models (DDPM) Tutorial\n",
    "This notebook gives you hands-on code for a *minimal* diffusion model (DDPM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-E95oWAL2gie",
   "metadata": {
    "id": "-E95oWAL2gie"
   },
   "source": [
    "We are now talking about models generating data, previously in the case of segmentation or image classification, we already had the data, we wanted to better locate it or categorize it. But now, Diffusion models are Generative AI, which means they are known for generating ie: creating data.\n",
    "\n",
    "Naturally, then the question arises: We live in a world with an abundance of data, then,\n",
    "\n",
    "**Why do we need to generate data?**\n",
    "\n",
    "- Having a lot of data doesn't necessarily mean that it is relevant data. Most real-world data are imbalances, biased or incomplete. So the problem is not the quantity of the data but the quality.\n",
    "\n",
    "- Diffusion Models gives us a way to simulate plausible data without new data collection.\n",
    "\n",
    "Generative Models are not just about creating new data, it is about **understanding** the structure of the data.\n",
    "\n",
    "For example, when asked to draw a picture of a smiley face, you are able to draw it as you understand that the structure of a smiley face is :)\n",
    "\n",
    "Similarly, when a model is said to be able to generate data, it means that, it is able to first understand the structure of the data and is then able to replicate it.\n",
    "\n",
    "**What are Diffusion Models?**\n",
    "\n",
    "- Diffusion Models are a family of probabilistic generative AI models that create new data - such as images, audio, video or 3D structures -  by learning how to reverse a gradual, step-by-step noise addition process.\n",
    "\n",
    "- **Overarching Idea:** Diffusion Models work by learning how to remove noise.\n",
    "\n",
    "Diffusion Models have a 2-stage process:\n",
    "\n",
    "- **Forward Process:**\n",
    "  - In the forward process, we gradually add small amounts of Gaussian Noise to an image:\n",
    "  \n",
    "    x₀ → x₁ → x₂ → ... → x_T\n",
    "\n",
    "    such that, at the end of the process, the image contains no information - just noise. Therefore, no learning happens here.\n",
    "\n",
    "- **Reverse Process:**\n",
    "  - In the reverse process, we train a neural network to answer:\n",
    "\n",
    "  > **Given a noisy image and a timestep, what noise was added?**\n",
    "\n",
    "  - Therefore, here, the model learns the how to remove the noise at every noise level, and by doing so, it implicitly learns the structure of the image distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BI8ujJq16xGp",
   "metadata": {
    "id": "BI8ujJq16xGp"
   },
   "source": [
    "The specific Diffusion Model we are building today is called a **Denoising Diffusion Probabilistic Model (DDPM)**.\n",
    "\n",
    "DDPM was introduced in 2020 and is the foundation of many SOTA Diffusion Models such as:\n",
    "- Stable Diffusion\n",
    "- Imagen\n",
    "- DALLE-style diffusion pipelines\n",
    "\n",
    "Today we are coding the simplified version of the same idea used in the SOTA image generators!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3fbc1",
   "metadata": {
    "id": "c4c3fbc1"
   },
   "source": [
    "## 1. Imports + Setup\n",
    "\n",
    "We are importing and installing the relevant packages needed.\n",
    "\n",
    "We also set the seed number to be 123 for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82603b42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82603b42",
    "outputId": "7aa74193-355b-4a1a-dfa3-80f0dbe13feb"
   },
   "outputs": [],
   "source": [
    "import os, math, random, time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb83284",
   "metadata": {
    "id": "bdb83284"
   },
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We build a data class - which is just a clean way to store configuration values - to define all the hyperparameters for our experiment in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58860885",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58860885",
    "outputId": "42aa8e6f-33b6-48fc-84f6-6efb49b54288"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    img_size: int = 32\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 2\n",
    "    lr: float = 2e-4\n",
    "    epochs: int = 150\n",
    "    train_subset: int = 20000  # reduce for speed (set None for full)\n",
    "    img_channels: int = 3\n",
    "\n",
    "    # Diffusion steps\n",
    "    T: int = ____  # TODO: 100 vs 200\n",
    "\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = ____  # TODO: 0.01 vs 0.02\n",
    "\n",
    "\n",
    "cfg = CFG()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0HLbo5-eh4",
   "metadata": {
    "id": "8f0HLbo5-eh4"
   },
   "source": [
    "- **Image Size:** Colorized-MNIST\n",
    "images are 32x32 in spatial resolution.\n",
    "\n",
    "- **Image Channels:** Tells the model that we are working with RGB images as the number of channels = 3.\n",
    "\n",
    "- **Batch Size:** Number of images we process in parallel.\n",
    "\n",
    "- **Num Workers:** Number of CPU workers to load the data in parallel - this affects speed and not learning.\n",
    "\n",
    "- **Learning Rate**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005cf4e",
   "metadata": {
    "id": "6005cf4e"
   },
   "source": [
    "## 3. Loading the Dataset - Colourized MNIST\n",
    "\n",
    "We prepare the training data so our diffusion model receives images in the exact format it expects - RGB, 32x32, and scaled to [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb8be7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5fb8be7",
    "outputId": "294111a9-4c4a-4957-f55e-c63b31277967"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ---- Paths (your existing path) ----\n",
    "DATA_DIR = r\"/content/colorized_mnist/colorized-MNIST-master\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"training\")\n",
    "TEST_DIR  = os.path.join(DATA_DIR, \"testing\")\n",
    "\n",
    "# ---- Image settings ----\n",
    "# Diffusion models want values in [-1, 1]\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std  = (0.5, 0.5, 0.5)\n",
    "\n",
    "# Keep transforms same for fairness (your existing rule)\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# ---- Dataset ----\n",
    "full_train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "test_ds       = datasets.ImageFolder(TEST_DIR,  transform=test_tfms)\n",
    "\n",
    "# Optional: class names (fine to keep)\n",
    "class_names = full_train_ds.classes\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# ---- Subset (from dump code) ----\n",
    "if cfg.train_subset is not None:\n",
    "    idx = np.random.permutation(len(full_train_ds))[:cfg.train_subset]\n",
    "    train_ds = Subset(full_train_ds, idx.tolist())\n",
    "else:\n",
    "    train_ds = full_train_ds\n",
    "\n",
    "# ---- DataLoader (from dump code) ----\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    drop_last=True,  # recommended for diffusion\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d7407a",
   "metadata": {
    "id": "73d7407a"
   },
   "source": [
    "### Visualize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cbce0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "e79cbce0",
    "outputId": "c7990fcb-7db5-4229-c1ab-aa99224f2ec1"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(x, title=\"\", n=64):\n",
    "    \"\"\"\n",
    "    x: Tensor [B,C,H,W] in [-1,1]\n",
    "    \"\"\"\n",
    "    x = (x.clamp(-1, 1) + 1) / 2  # -> [0,1]\n",
    "\n",
    "    grid = make_grid(x[:n], nrow=int(n**0.5))\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "xb, _ = next(iter(train_loader))\n",
    "show_images(xb, \"Colorized MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebaa78",
   "metadata": {
    "id": "c0ebaa78"
   },
   "source": [
    "## 3. Forward Diffusion\n",
    "We define a noise schedule βₜ and compute:\n",
    "\n",
    "- αₜ = 1 − βₜ\n",
    "- \\bar{α}ₜ = Πₛ≤ₜ αₛ\n",
    "\n",
    "Sampling noisy image:\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon,\\quad \\epsilon\\sim\\mathcal{N}(0,I)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb01268",
   "metadata": {
    "id": "1cb01268"
   },
   "outputs": [],
   "source": [
    "# Create beta schedule\n",
    "betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.T, device=device)  # [T]\n",
    "alphas = 1.0 - betas\n",
    "alphas_bar = torch.cumprod(alphas, dim=0)  # [T]\n",
    "\n",
    "def q_sample(x0, t, noise=None):\n",
    "    \"\"\"Sample x_t given x_0 at timestep t (0-indexed).\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "    # Gather alpha_bar_t for each element in batch\n",
    "    a_bar = alphas_bar[t].view(-1, 1, 1, 1)  # [B,1,1,1]\n",
    "    return torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * noise, noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e1708",
   "metadata": {
    "id": "e77e1708"
   },
   "source": [
    "### See what forward diffusion does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec300e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "fec300e6",
    "outputId": "4e4d7bfe-2f74-4b64-9016-b57ed687cd8f"
   },
   "outputs": [],
   "source": [
    "x0, _ = next(iter(train_loader))\n",
    "x0 = x0.to(device)[:16]\n",
    "\n",
    "ts = torch.tensor([0, 25, 50, 100, 199], device=device)\n",
    "outs = []\n",
    "for t in ts:\n",
    "    xt, _ = q_sample(x0, torch.full((x0.size(0),), t, device=device, dtype=torch.long))\n",
    "    outs.append(xt)\n",
    "\n",
    "for i, t in enumerate(ts.tolist()):\n",
    "    show_images(outs[i], title=f\"x_t at t={t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g-Mkyh0F3IJp",
   "metadata": {
    "id": "g-Mkyh0F3IJp"
   },
   "source": [
    "In the forward diffusion process, we progressively add Gaussian noise to an image so that it transitions smoothly from structured data to pure noise in a mathematically controlled way, as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24417a78",
   "metadata": {
    "id": "24417a78"
   },
   "source": [
    "## 4) Noise-Predictor Network ε_θ(xₜ, t)\n",
    "We train a network to predict the noise that was added.\n",
    "\n",
    "Loss:\n",
    "$$\\mathcal{L}=\\mathbb{E}_{t, x_0, \\epsilon}[\\|\\epsilon-\\epsilon_\\theta(x_t,t)\\|^2]$$\n",
    "\n",
    "For MNIST we can use a tiny U-Net-like CNN with a timestep embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a5cdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "126a5cdf",
    "outputId": "7b43d295-e770-42ee-e86b-696200b7fda3"
   },
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: [B] integer timesteps\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / (half - 1))\n",
    "        args = t.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
    "        return emb  # [B, dim]\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, t_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(t_dim, out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.res = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.act(self.bn1(self.conv1(x)))\n",
    "        # Add time embedding (broadcast over H,W)\n",
    "        t = self.time_mlp(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        h = h + t\n",
    "        h = self.act(self.bn2(self.conv2(h)))\n",
    "        return h + self.res(x)\n",
    "\n",
    "class TinyUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, t_dim=64, base=64):\n",
    "        super().__init__()\n",
    "        self.t_embed = SinusoidalTimeEmbedding(t_dim)\n",
    "        self.t_mlp = nn.Sequential(\n",
    "            nn.Linear(t_dim, t_dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_dim*4, t_dim)\n",
    "        )\n",
    "\n",
    "        # ↓↓↓ change: 1 -> in_ch\n",
    "        self.down1 = ResBlock(in_ch, base, t_dim)\n",
    "        self.down2 = ResBlock(base, base*2, t_dim)\n",
    "        self.pool  = nn.MaxPool2d(2)\n",
    "\n",
    "        self.mid   = ResBlock(base*2, base*2, t_dim)\n",
    "\n",
    "        self.up    = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.up1   = ResBlock(base*2 + base, base, t_dim)\n",
    "\n",
    "        # ↓↓↓ change: 1 -> out_ch\n",
    "        self.out   = nn.Conv2d(base, out_ch, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.t_mlp(self.t_embed(t))\n",
    "\n",
    "        h1 = self.down1(x, t_emb)              # [B, base, H, W]\n",
    "        h2 = self.down2(self.pool(h1), t_emb)  # [B, 2base, H/2, W/2]\n",
    "        h  = self.mid(h2, t_emb)\n",
    "\n",
    "        h  = self.up(h)                        # [B, 2base, H, W]\n",
    "        h  = torch.cat([h, h1], dim=1)         # skip\n",
    "        h  = self.up1(h, t_emb)\n",
    "        return self.out(h)                     # predict noise epsilon\n",
    "\n",
    "eps_model = TinyUNet(in_ch=3, out_ch=3, t_dim=64, base=64).to(device)\n",
    "optimizer = torch.optim.Adam(eps_model.parameters(), lr=cfg.lr)\n",
    "\n",
    "sum(p.numel() for p in eps_model.parameters())/1e6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a370e",
   "metadata": {
    "id": "c13a370e"
   },
   "source": [
    "## 5) Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be605303",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be605303",
    "outputId": "a44bc291-6c51-4946-c952-edbfbdc02e23"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    eps_model.train()\n",
    "    loss_meter = 0.0\n",
    "\n",
    "    for x0, _ in train_loader:\n",
    "        x0 = x0.to(device)\n",
    "        B = x0.size(0)\n",
    "\n",
    "        t = torch.randint(0, cfg.T, (B,), device=device, dtype=torch.long)\n",
    "        xt, noise = q_sample(x0, t)\n",
    "        noise_pred = eps_model(xt, t)\n",
    "\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_meter += loss.item()\n",
    "\n",
    "    return loss_meter / len(train_loader)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1, cfg.epochs+1):\n",
    "    t0 = time.time()\n",
    "    loss = train_one_epoch()\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch:02d} | loss {loss:.4f} | {(time.time()-t0):.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5161a95",
   "metadata": {
    "id": "b5161a95"
   },
   "source": [
    "### Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605ceb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "d605ceb0",
    "outputId": "4605fc74-9dcb-4f48-b118-74278868800a"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"DDPM training loss (MSE on noise)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646507ba",
   "metadata": {
    "id": "646507ba"
   },
   "source": [
    "## 6) Sampling (Reverse Diffusion)\n",
    "We start from pure noise **x_T ~ N(0,I)** and step backwards:\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\,\\epsilon_\\theta(x_t,t)\\right) + \\sigma_t z$$\n",
    "with $z\\sim N(0,I)$ for stochasticity.\n",
    "\n",
    "This is the classic DDPM ancestral sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf7f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "eadf7f40",
    "outputId": "11dc8739-cbf6-4ff7-f596-f869e6822f06"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Visualizer (RGB-safe)\n",
    "# ---------------------------\n",
    "def show_images(x, title=\"\", n=64):\n",
    "    \"\"\"\n",
    "    x: [B,C,H,W] in [-1,1]\n",
    "    \"\"\"\n",
    "    x = (x.clamp(-1, 1) + 1) / 2  # -> [0,1]\n",
    "    grid = make_grid(x[:n], nrow=int(n**0.5))\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Reverse step: p(x_{t-1} | x_t)\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def p_sample(x, t):\n",
    "    \"\"\"\n",
    "    Sample x_{t-1} from x_t using the DDPM update.\n",
    "\n",
    "    x: [B,C,H,W] in roughly [-1,1]\n",
    "    t: integer scalar in [0, T-1]\n",
    "    \"\"\"\n",
    "    B = x.size(0)\n",
    "    t_batch = torch.full((B,), t, device=x.device, dtype=torch.long)\n",
    "\n",
    "    # Predict noise eps_theta(x_t, t)\n",
    "    eps = eps_model(x, t_batch)  # [B,C,H,W]\n",
    "\n",
    "    # Make scalars broadcastable to [B,C,H,W]\n",
    "    beta_t      = betas[t].view(1, 1, 1, 1)\n",
    "    alpha_t     = alphas[t].view(1, 1, 1, 1)\n",
    "    alpha_bar_t = alphas_bar[t].view(1, 1, 1, 1)\n",
    "\n",
    "    # DDPM mean (mu_theta)\n",
    "    mean = (1.0 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1.0 - alpha_bar_t)) * eps)\n",
    "\n",
    "    if t == 0:\n",
    "        return mean  # final sample\n",
    "\n",
    "    # Simple variance choice: sigma_t^2 = beta_t\n",
    "    z = torch.randn_like(x)\n",
    "    return mean + torch.sqrt(beta_t) * z\n",
    "\n",
    "# ---------------------------\n",
    "# Full sampling loop\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def sample(n=64):\n",
    "    eps_model.eval()\n",
    "\n",
    "    # IMPORTANT: RGB + 32x32\n",
    "    x = torch.randn(n, cfg.img_channels, cfg.img_size, cfg.img_size, device=device)\n",
    "\n",
    "    for t in reversed(range(cfg.T)):\n",
    "        x = p_sample(x, t)\n",
    "\n",
    "    return x\n",
    "\n",
    "# ---------------------------\n",
    "# Generate + visualize\n",
    "# ---------------------------\n",
    "samples = sample(64)\n",
    "show_images(samples, title=\"Generated samples (DDPM, Colorized MNIST)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Student Task: Reflection\n",
    "1. What does DDPM predict during training?\n",
    "2. Why do we denoise over many steps?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6U7UQ-cXQceC",
   "metadata": {
    "id": "6U7UQ-cXQceC"
   },
   "source": [
    "## Hyperparameter Sensitivity Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B_18toC9WdVq",
   "metadata": {
    "id": "B_18toC9WdVq"
   },
   "source": [
    "### Why decreasing T looks noisier\n",
    "\n",
    "compare forward noise at the same fraction of diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jGqoDc7WQeM7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jGqoDc7WQeM7",
    "outputId": "c3d60dba-65a7-49a1-af5d-f4c6ecca675e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def build_schedules(T, beta_start, beta_end, device):\n",
    "    betas = torch.linspace(beta_start, beta_end, T, device=device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "    return betas, alphas, alphas_bar\n",
    "\n",
    "@torch.no_grad()\n",
    "def q_sample_with_schedule(x0, t, alphas_bar):\n",
    "    eps = torch.randn_like(x0)\n",
    "    a_bar = alphas_bar[t].view(-1,1,1,1)\n",
    "    xt = torch.sqrt(a_bar) * x0 + torch.sqrt(1.0 - a_bar) * eps\n",
    "    return xt\n",
    "\n",
    "def show_grid(x, title=\"\", n=16):\n",
    "    x = (x.clamp(-1,1) + 1)/2\n",
    "    grid = make_grid(x[:n], nrow=4)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(grid.permute(1,2,0).cpu())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# pick one batch\n",
    "x0, _ = next(iter(train_loader))\n",
    "x0 = x0.to(device)\n",
    "\n",
    "# compare different T at same FRACTIONS\n",
    "Ts = [50, 100, 200]\n",
    "fractions = [0.0, 0.25, 0.5, 0.75, 0.99]\n",
    "\n",
    "for T_ in Ts:\n",
    "    betas_, alphas_, alphas_bar_ = build_schedules(T_, cfg.beta_start, cfg.beta_end, device)\n",
    "    for f in fractions:\n",
    "        t = int(f * (T_-1))\n",
    "        t_batch = torch.full((x0.size(0),), t, device=device, dtype=torch.long)\n",
    "        xt = q_sample_with_schedule(x0, t_batch, alphas_bar_)\n",
    "        show_grid(xt, title=f\"Forward diffusion | T={T_} | t={t} ({int(f*100)}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IGGn9DTfWWic",
   "metadata": {
    "id": "IGGn9DTfWWic"
   },
   "source": [
    "## More gradient steps = better samples” (without waiting forever)\n",
    "\n",
    "Diffusion models learn gradually:\n",
    "first background/color stats, then blobs, then strokes, then sharp digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qjv8MzfQQfyl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qjv8MzfQQfyl",
    "outputId": "3807bcaa-c57c-4d5b-a338-469ec38eb2e6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_steps_with_snapshots(eps_model, optimizer, train_loader, snap_steps=(50, 200, 500), max_steps=500):\n",
    "    eps_model.train()\n",
    "    losses = []\n",
    "    snaps = {}\n",
    "\n",
    "    it = iter(train_loader)\n",
    "    for step in range(1, max_steps+1):\n",
    "        try:\n",
    "            x0, _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(train_loader)\n",
    "            x0, _ = next(it)\n",
    "\n",
    "        x0 = x0.to(device)\n",
    "        B = x0.size(0)\n",
    "        t = torch.randint(0, cfg.T, (B,), device=device, dtype=torch.long)\n",
    "\n",
    "        eps = torch.randn_like(x0)\n",
    "        x_t, eps = q_sample(x0, t, eps)         # your existing q_sample\n",
    "        eps_hat = eps_model(x_t, t)\n",
    "        loss = F.mse_loss(eps_hat, eps)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(eps_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if step in snap_steps:\n",
    "            eps_model.eval()\n",
    "            s = sample(36)                      # your patched RGB sample()\n",
    "            snaps[step] = s.detach().cpu()\n",
    "            eps_model.train()\n",
    "\n",
    "    return losses, snaps\n",
    "\n",
    "# Run a quick demo (fast)\n",
    "eps_model = TinyUNet(in_ch=cfg.img_channels, out_ch=cfg.img_channels, t_dim=64, base=64).to(device)\n",
    "optimizer = torch.optim.Adam(eps_model.parameters(), lr=2e-4)\n",
    "\n",
    "losses, snaps = train_steps_with_snapshots(eps_model, optimizer, train_loader,\n",
    "                                           snap_steps=(50, 200, 500), max_steps=500)\n",
    "\n",
    "# Loss curve\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training loss vs gradient steps (quick demo)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"MSE on noise ε\")\n",
    "plt.show()\n",
    "\n",
    "# Sample snapshots\n",
    "for k in sorted(snaps.keys()):\n",
    "    show_grid(snaps[k], title=f\"Samples after {k} gradient steps\", n=36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6mNcEwB6WOtr",
   "metadata": {
    "id": "6mNcEwB6WOtr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
