{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi4D4Q7leC0a"
   },
   "source": [
    "# Image Processing with Deep Learning\n",
    "<a href=\"https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2026/blob/main/SESSION3/session3_part1_instructor.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "Welcome to the third session of deep learning bootcamp.  Today we are going to learn how computers \"see\" images and how we can use deep learning for image processing.  Feel free to jot down any notes you have from today's session in this notebook and please feel free to modify and experiment with the code during today's exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: How Do Computers See Images?\n",
    "\n",
    "Before we dive into deep learning, let's understand a fundamental concept: **computers don't see images the way we do**. To a computer, every image is just a grid of numbers!\n",
    "\n",
    "Let's see this for ourselves using handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import the tools we need\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Download the MNIST dataset (handwritten digits 0-9)\n",
    "mnist_data = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "print(\"Dataset downloaded successfully!\")\n",
    "print(f\"We have {len(mnist_data)} images of handwritten digits to work with.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to colourize MNIST digits\n",
    "def colourize_mnist(image, colour_seed=None):\n",
    "    \"\"\"\n",
    "    Convert a grayscale MNIST image to a coloured version.\n",
    "    Each digit gets a random colour applied to it.\n",
    "    \"\"\"\n",
    "    if colour_seed is not None:\n",
    "        np.random.seed(colour_seed)\n",
    "    \n",
    "    # Get the grayscale image as numpy array\n",
    "    img_array = image.squeeze().numpy()\n",
    "    \n",
    "    # Create RGB image\n",
    "    coloured_img = np.zeros((3, 28, 28))\n",
    "    \n",
    "    # Generate a random colour (RGB values between 0.3 and 1.0 for visibility)\n",
    "    colour = np.random.uniform(0.3, 1.0, 3)\n",
    "    \n",
    "    # Apply the colour to each channel\n",
    "    for i in range(3):\n",
    "        coloured_img[i] = img_array * colour[i]\n",
    "    \n",
    "    return torch.tensor(coloured_img, dtype=torch.float32)\n",
    "\n",
    "print(\"colourization function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing an Image as Humans Do\n",
    "\n",
    "Let's first look at an image the way we normally would:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one image from the dataset and colourize it\n",
    "gray_image, label = mnist_data[0]  # The first image\n",
    "image = colourize_mnist(gray_image, colour_seed=42)\n",
    "\n",
    "# Display it\n",
    "plt.figure(figsize=(5, 5))\n",
    "# Convert from CHW to HWC format for displaying\n",
    "plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "plt.title(f\"This is the digit: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"You can see this is a handwritten '{label}'\")\n",
    "print(f\"The image has 3 colour channels: Red, Green, and Blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing an Image as Computers Do: Just Numbers!\n",
    "\n",
    "Now let's see what the computer actually \"sees\". \n",
    "\n",
    "**colour images have 3 layers** (Red, Green, Blue). Each pixel has 3 numbers - one for each colour!\n",
    "- Each number ranges from 0.0 (no colour) to 1.0 (full colour intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the three colour channels separately\n",
    "red_channel = image[0].numpy()\n",
    "green_channel = image[1].numpy()\n",
    "blue_channel = image[2].numpy()\n",
    "\n",
    "print(\"The image has 3 separate colour layers:\")\n",
    "print(f\"- Red channel: {red_channel.shape[0]}Ã—{red_channel.shape[1]} numbers\")\n",
    "print(f\"- Green channel: {green_channel.shape[0]}Ã—{green_channel.shape[1]} numbers\") \n",
    "print(f\"- Blue channel: {blue_channel.shape[0]}Ã—{blue_channel.shape[1]} numbers\")\n",
    "print(f\"\\nTotal: 3 Ã— 28 Ã— 28 = {3*28*28} numbers to represent this one small image!\")\n",
    "\n",
    "# Show a small section of numbers from each channel\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Here's a 5Ã—5 section from the RED channel (top-left corner):\")\n",
    "print(\"=\"*60)\n",
    "print(np.round(red_channel[10:15, 10:15], 2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Here's the same section from the GREEN channel:\")\n",
    "print(\"=\"*60)\n",
    "print(np.round(green_channel[10:15, 10:15], 2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Here's the same section from the BLUE channel:\")\n",
    "print(\"=\"*60)\n",
    "print(np.round(blue_channel[10:15, 10:15], 2))\n",
    "\n",
    "print(\"\\nðŸ’¡ 0.0 = no colour, 1.0 = full colour intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Colour Channels\n",
    "\n",
    "Let's see each colour channel separately to understand how they combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Show each channel\n",
    "axes[0].imshow(red_channel, cmap='Reds', vmin=0, vmax=1)\n",
    "axes[0].set_title('Red Channel\\n(How much red at each pixel)', fontsize=11)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(green_channel, cmap='Greens', vmin=0, vmax=1)\n",
    "axes[1].set_title('Green Channel\\n(How much green at each pixel)', fontsize=11)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(blue_channel, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[2].set_title('Blue Channel\\n(How much blue at each pixel)', fontsize=11)\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Show combined\n",
    "axes[3].imshow(image.permute(1, 2, 0).numpy())\n",
    "axes[3].set_title('All Channels Combined\\n(The final colour image!)', fontsize=11, fontweight='bold')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe computer mixes Red + Green + Blue numbers to create the final colours!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Numbers to Pixels\n",
    "\n",
    "Let's zoom into a tiny 6Ã—6 pixel section and see both the numbers AND the colours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small 6Ã—6 section\n",
    "section_size = 6\n",
    "start_row, start_col = 10, 10\n",
    "\n",
    "small_section = image[:, start_row:start_row+section_size, start_col:start_col+section_size]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Visual representation (zoomed in)\n",
    "ax1.imshow(small_section.permute(1, 2, 0).numpy())\n",
    "ax1.set_title(f'What We See: A {section_size}Ã—{section_size} pixel section\\n(Zoomed in!)', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Add grid to show individual pixels\n",
    "for i in range(section_size + 1):\n",
    "    ax1.axhline(i - 0.5, color='white', linewidth=0.5, alpha=0.5)\n",
    "    ax1.axvline(i - 0.5, color='white', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Right: The actual numbers for ONE pixel\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "\n",
    "# Show the RGB values for the center pixel\n",
    "center_pixel = small_section[:, section_size//2, section_size//2]\n",
    "r_val = center_pixel[0].item()\n",
    "g_val = center_pixel[1].item()\n",
    "b_val = center_pixel[2].item()\n",
    "\n",
    "text = f\"\"\"What the Computer Sees for ONE Pixel:\n",
    "\n",
    "Each pixel needs 3 numbers (RGB):\n",
    "\n",
    "ðŸ”´ Red value:   {r_val:.3f}\n",
    "ðŸŸ¢ Green value: {g_val:.3f}\n",
    "ðŸ”µ Blue value:  {b_val:.3f}\n",
    "\n",
    "These 3 numbers create the pixel's colour!\n",
    "\n",
    "For the full {section_size}Ã—{section_size} section:\n",
    "â†’ {section_size}Ã—{section_size}Ã—3 = {section_size*section_size*3} numbers total\n",
    "\n",
    "For the complete 28Ã—28 image:\n",
    "â†’ 28Ã—28Ã—3 = 2,352 numbers total!\n",
    "\"\"\"\n",
    "\n",
    "ax2.text(0.5, 5, text, fontsize=11, family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "         verticalalignment='center')\n",
    "ax2.set_title('What the Computer Sees: Numbers!', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Colour images need 3Ã— as many numbers as grayscale.\")\n",
    "\n",
    "print(\"â€¢ Grayscale: 1 number per pixel (just brightness)\")\n",
    "print(\"â€¢ colour: 3 numbers per pixel (Red, Green, Blue)\")\n",
    "print(\"\\nThis is why colour images take more storage and processing power!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Matters for Deep Learning\n",
    "\n",
    "Understanding that images are numbers is crucial because:\n",
    "- Computers can only do math with numbers\n",
    "- Deep learning works by doing mathematical operations on these numbers\n",
    "- When we \"process\" an image, we're really just transforming these numbers in smart ways\n",
    "- **Colour images have 3 channels**, so we need to process all three!\n",
    "\n",
    "Now let's see how we can use this understanding to detect patterns in images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avWdnFiWe9Py"
   },
   "source": [
    "## Excercise 1: Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fITfC3HAwrh"
   },
   "source": [
    "Now we will perform edge detection on an image using manually designed kernel.  Edges are where the brightness changes suddenly (like going from black to white). We can find them by comparing neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get an Image to Work With\n",
    "\n",
    "Let's pick a digit that has clear edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "ECo_5ZB37cmd",
     "outputId": "d9f32cec-56f1-48d3-e39d-4514b759f08b"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "ECo_5ZB37cmd",
     "outputId": "19df6be9-cc2a-411d-88d9-9ff5c7cbd241"
    }
   },
   "outputs": [],
   "source": [
    "# Let's find a nice clear digit to work with\n",
    "# We'll look for a '4' which has good vertical and horizontal edges\n",
    "for i in range(len(mnist_data)):\n",
    "    gray_img, lbl = mnist_data[i]\n",
    "    if lbl == 4:\n",
    "        sample_gray = gray_img\n",
    "        sample_label = lbl\n",
    "        break\n",
    "\n",
    "# colourize it\n",
    "sample_image = colourize_mnist(sample_gray, colour_seed=123)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_image.permute(1, 2, 0).numpy())\n",
    "plt.title(f\"Our Sample Digit: {sample_label}\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Convert to the format we need for processing\n",
    "img = sample_image.float()\n",
    "print(f\"Image shape: {img.shape} = [3 channels, 28 height, 28 width]\")\n",
    "print(\"Image ready for edge detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Detecting Vertical Edges\n",
    "\n",
    "A vertical edge is where the image goes from dark to light (or light to dark) as you move left to right.\n",
    "\n",
    "For **colour images**, we need to apply the filter to each colour channel separately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vertical edge detector for colour images\n",
    "# Notice: in_channels=3 for RGB!\n",
    "vertical_filter = torch.nn.Conv2d(\n",
    "    in_channels=3,      # We have 3 colour channels (RGB)\n",
    "    out_channels=3,     # We want 3 outputs (one per colour)\n",
    "    kernel_size=3,      # We look at 3Ã—3 pixels at a time\n",
    "    bias=False,\n",
    "    groups=3            # Process each colour independently\n",
    ")\n",
    "\n",
    "# Set the Sobel filter pattern for each channel\n",
    "sobel_kernel = torch.Tensor([[\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "]])\n",
    "\n",
    "# Apply the same pattern to all 3 colour channels\n",
    "weights = torch.zeros(3, 1, 3, 3)\n",
    "for i in range(3):\n",
    "    weights[i, 0] = sobel_kernel\n",
    "\n",
    "vertical_filter.weight = torch.nn.Parameter(weights)\n",
    "\n",
    "print(\"Vertical edge filter created for colour images!\")\n",
    "print(f\"Filter shape: {vertical_filter.weight.shape}\")\n",
    "print(\"  â†’ [3 output channels, 1 input per group, 3Ã—3 kernel]\")\n",
    "print(\"\\nThe same edge-detection pattern is applied to Red, Green, and Blue separately.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter to find vertical edges\n",
    "vertical_edges = vertical_filter(img.unsqueeze(0))\n",
    "vertical_edges = vertical_edges.detach().squeeze()\n",
    "\n",
    "# # Show the results\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# # Top row: Original channels\n",
    "# axes[0, 0].imshow(img[0].numpy(), cmap='Reds')\n",
    "# axes[0, 0].set_title('Original - Red Channel', fontsize=11)\n",
    "# axes[0, 0].axis('off')\n",
    "\n",
    "# axes[0, 1].imshow(img[1].numpy(), cmap='Greens')\n",
    "# axes[0, 1].set_title('Original - Green Channel', fontsize=11)\n",
    "# axes[0, 1].axis('off')\n",
    "\n",
    "# axes[0, 2].imshow(img[2].numpy(), cmap='Blues')\n",
    "# axes[0, 2].set_title('Original - Blue Channel', fontsize=11)\n",
    "# axes[0, 2].axis('off')\n",
    "\n",
    "# # Bottom row: Edges detected in each channel\n",
    "# axes[1, 0].imshow(vertical_edges[0].numpy(), cmap='Reds')\n",
    "# axes[1, 0].set_title('Vertical Edges - Red Channel', fontsize=11, fontweight='bold')\n",
    "# axes[1, 0].axis('off')\n",
    "\n",
    "# axes[1, 1].imshow(vertical_edges[1].numpy(), cmap='Greens')\n",
    "# axes[1, 1].set_title('Vertical Edges - Green Channel', fontsize=11, fontweight='bold')\n",
    "# axes[1, 1].axis('off')\n",
    "\n",
    "# axes[1, 2].imshow(vertical_edges[2].numpy(), cmap='Blues')\n",
    "# axes[1, 2].set_title('Vertical Edges - Blue Channel', fontsize=11, fontweight='bold')\n",
    "# axes[1, 2].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Also show the combined colour result\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.imshow(img.permute(1, 2, 0).numpy())\n",
    "ax1.set_title('Original colour Image', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Normalize for display\n",
    "edges_normalized = (vertical_edges - vertical_edges.min()) / (vertical_edges.max() - vertical_edges.min())\n",
    "ax2.imshow(edges_normalized.permute(1, 2, 0).numpy())\n",
    "ax2.set_title('Vertical Edges Detected (All Channels)', fontsize=12, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the vertical parts of the digit '4' are highlighted!\")\n",
    "print(\"The edge detection worked on all three colour channels at once.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Your Turn - Detect Horizontal Edges!\n",
    "\n",
    "Can you create a filter to detect horizontal edges in the image?\n",
    "\n",
    "**Hint:** For horizontal edges, we compare pixels above with pixels below (instead of left vs. right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "oTa0RB3rBdPq",
     "outputId": "9ffc4b82-2c84-43f8-f15c-ed407f0aef25"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "oTa0RB3rBdPq",
     "outputId": "b9c1971c-2d28-4f6d-88ae-660fa75fbcc0"
    }
   },
   "outputs": [],
   "source": [
    "# Create a horizontal edge detector for colour images\n",
    "horizontal_filter = torch.nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    kernel_size=3,\n",
    "    bias=False,\n",
    "    groups=3\n",
    ")\n",
    "\n",
    "# TODO: Fill in the pattern for horizontal edges\n",
    "# Hint: top row should be positive, bottom row should be negative\n",
    "horizontal_kernel = torch.Tensor([[\n",
    "    # FILL IN HERE\n",
    "]])\n",
    "\n",
    "# Apply the same pattern to all 3 colour channels\n",
    "weights = torch.zeros(3, 1, 3, 3)\n",
    "for i in range(3):\n",
    "    weights[i, 0] = horizontal_kernel\n",
    "\n",
    "horizontal_filter.weight = torch.nn.Parameter(weights)\n",
    "\n",
    "# Apply the filter\n",
    "horizontal_edges = horizontal_filter(img.unsqueeze(0))\n",
    "horizontal_edges = horizontal_edges.detach().squeeze()\n",
    "\n",
    "# Show the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.imshow(img.permute(1, 2, 0).numpy())\n",
    "ax1.set_title('Original colour Image', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Normalize for display\n",
    "edges_normalized = (horizontal_edges - horizontal_edges.min()) / (horizontal_edges.max() - horizontal_edges.min())\n",
    "ax2.imshow(edges_normalized.permute(1, 2, 0).numpy())\n",
    "ax2.set_title('Horizontal Edges Detected (All Channels)', fontsize=12, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Great job! The horizontal parts of the '4' should now be highlighted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbUogKP4CT7u"
   },
   "source": [
    "### Step 4: Combining Both Edge Types\n",
    "\n",
    "Now let's combine vertical and horizontal edges to see ALL the edges in our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "W8DcbalTe4N4",
     "outputId": "572c8a23-7ddd-4c38-b35e-89851e2a25ae"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 435
     },
     "id": "W8DcbalTe4N4",
     "outputId": "7e9f1d23-54b2-4140-ee61-73406f27e61b"
    }
   },
   "outputs": [],
   "source": [
    "# Combine the edge maps (for each colour channel)\n",
    "all_edges = torch.sqrt(horizontal_edges**2 + vertical_edges**2)\n",
    "\n",
    "# Display all stages together\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "axes[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Normalize vertical edges for display\n",
    "v_norm = (vertical_edges - vertical_edges.min()) / (vertical_edges.max() - vertical_edges.min())\n",
    "axes[1].imshow(v_norm.permute(1, 2, 0).numpy())\n",
    "axes[1].set_title('Vertical Edges', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Normalize horizontal edges for display\n",
    "h_norm = (horizontal_edges - horizontal_edges.min()) / (horizontal_edges.max() - horizontal_edges.min())\n",
    "axes[2].imshow(h_norm.permute(1, 2, 0).numpy())\n",
    "axes[2].set_title('Horizontal Edges', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Normalize combined edges for display\n",
    "all_norm = (all_edges - all_edges.min()) / (all_edges.max() - all_edges.min())\n",
    "axes[3].imshow(all_norm.permute(1, 2, 0).numpy())\n",
    "axes[3].set_title('All Edges Combined!', fontsize=12, fontweight='bold')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Amazing! You've just detected edges in an image!\")\n",
    "\n",
    "print(\"\\nWhat we learned:\")\n",
    "print(\"1. colour images are grids of numbers - 3 numbers per pixel (R, G, B)\")\n",
    "print(\"2. We can transform these numbers using filters\")\n",
    "print(\"3. Filters work on each colour channel separately\")\n",
    "print(\"4. Different filters detect different patterns (vertical vs horizontal edges)\")\n",
    "print(\"5. This is the foundation of how neural networks 'see' colour images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1TqRt2Jjl-u"
   },
   "source": [
    "### Understanding What Just Happened with colour\n",
    "\n",
    "Let's break down the process:\n",
    "\n",
    "1. **Original colour Image**: We started with a digit made of 3 layers of numbers (R, G, B)\n",
    "2. **Apply Filter to Each Channel**: We slid a 3Ã—3 pattern over each colour layer separately\n",
    "3. **Math Operation**: At each position, we multiplied and added numbers\n",
    "4. **3 Result Channels**: We got edges detected in Red, Green, and Blue separately\n",
    "5. **Combine**: We can view them together as a colour edge map!\n",
    "\n",
    "**The Key Difference from Grayscale:**\n",
    "- Grayscale: 1 filter â†’ 1 output\n",
    "- colour: 3 filters (one per channel) â†’ 3 outputs\n",
    "- This is why we used `in_channels=3` and `out_channels=3`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-tyBOa_kEJz"
   },
   "source": [
    "## Exercise 2: Understanding Convolution Parameters\n",
    "\n",
    "The filters we used above are called **convolutions**. Let's understand how to control them for colour images.\n",
    "\n",
    "### 2a: Input and Output Channels\n",
    "\n",
    "- **Input channels**: How many \"layers\" the input has (grayscale=1, **colour=3**)\n",
    "- **Output channels**: How many different feature maps we want to create\n",
    "\n",
    "For colour images, we can:\n",
    "1. Process each colour separately (like we did above)\n",
    "2. Mix colours together to create new features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 453
     },
     "id": "YU1wc6Ykjker",
     "outputId": "267e6c8a-1474-4154-9dac-1fad752e8f99"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 452
     },
     "id": "YU1wc6Ykjker",
     "outputId": "f28a8656-cc5c-4dd5-a4a7-b040d066a129"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a filter that takes 3 inputs (RGB) and produces 6 outputs\n",
    "# This means we're learning 6 different patterns from the colour image!\n",
    "multi_filter = torch.nn.Conv2d(\n",
    "    in_channels= # TODO: Fill how many channels we need for our coloured MNIST images\n",
    "    out_channels=6,     # We want to detect 6 different patterns\n",
    "    kernel_size=3,      # Each filter looks at 3Ã—3 pixels\n",
    "    padding=1           # Keep the same size\n",
    ")\n",
    "\n",
    "# Apply it to our coloured image\n",
    "x = img.unsqueeze(0)  # Add batch dimension\n",
    "output = multi_filter(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"  - Batch size: {x.shape[0]} (how many images at once)\")\n",
    "print(f\"  - Channels: {x.shape[1]} (colour = 3: R, G, B)\")\n",
    "print(f\"  - Height: {x.shape[2]} pixels\")\n",
    "print(f\"  - Width: {x.shape[3]} pixels\")\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"  - We now have {output.shape[1]} feature channels!\")\n",
    "print(f\"  - Each filter learned to combine R, G, B in different ways\")\n",
    "\n",
    "# Visualize what each of the 6 filters detected\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Show original\n",
    "axes[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title('Original colour Image', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show each of the 6 filter outputs\n",
    "for i in range(6):\n",
    "    axes[i+1].imshow(output[0, i].detach().numpy(), cmap='viridis')\n",
    "    axes[i+1].set_title(f'Feature Map {i+1}\\n(combines R+G+B)', fontsize=10)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[7].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Each feature map learned different patterns by mixing the colour channels!\")\n",
    "print(\"In deep learning, these patterns would be trained to detect useful features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ivP-AT4eGzB"
   },
   "source": [
    "### 2b: kernel size\n",
    "\n",
    "**Kernel size** determines how much of the image the filter looks at:\n",
    "- Small kernels (3Ã—3): Detect fine details\n",
    "- Large kernels (7Ã—7): Detect larger patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "hu-Tnv_aeGHj",
     "outputId": "e9f25139-0dc3-461f-fdb0-6c8ff46f56cf"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "hu-Tnv_aeGHj",
     "outputId": "cbee0d14-ab9d-4c78-9ea5-433c52dbcaa6"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try different kernel sizes on our colour image\n",
    "kernel_sizes = [3, 5, 7]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(kernel_sizes) + 1, figsize=(16, 4))\n",
    "\n",
    "# Show original\n",
    "axes[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title('Original\\ncolour Image', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Try each kernel size\n",
    "for idx, ksize in enumerate(kernel_sizes):\n",
    "    conv = torch.nn.Conv2d(\n",
    "        in_channels=3,      # Colour input\n",
    "        out_channels=3,     # Colour output\n",
    "        kernel_size=ksize,\n",
    "        padding=ksize//2,   # Keep same size\n",
    "        groups=3            # Process each channel separately\n",
    "    )\n",
    "    \n",
    "    output = conv(img.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    output_norm = (output - output.min()) / (output.max() - output.min())\n",
    "    \n",
    "    axes[idx + 1].imshow(output_norm.permute(1, 2, 0).detach().numpy())\n",
    "    axes[idx + 1].set_title(f'Kernel: {ksize}Ã—{ksize}', fontsize=11, fontweight='bold')\n",
    "    axes[idx + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how larger kernels create smoother, more blurred results.\")\n",
    "print(\"They 'see' more of the image at once, detecting larger patterns.\")\n",
    "print(\"\\nThis works exactly the same for colour as it does for grayscale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_tax8OKeYn9"
   },
   "source": [
    "### 2c: Stride\n",
    "\n",
    "**Stride** is how many pixels the filter jumps each time:\n",
    "- Stride=1: Move one pixel at a time\n",
    "- Stride=2: Skip every other pixel (makes output smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "GolkUgaVekMC",
     "outputId": "390a6502-f1fd-426a-f970-a9515070afeb"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "GolkUgaVekMC",
     "outputId": "8809551a-32f0-4484-fbed-8504e04698ea"
    }
   },
   "outputs": [],
   "source": [
    "# Compare different strides on colour images\n",
    "strides = [1, 2, 4]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(strides) + 1, figsize=(14, 4))\n",
    "\n",
    "axes[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(f'Original\\n{img.shape[1]}Ã—{img.shape[2]}', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for idx, stride in enumerate(strides):\n",
    "    conv = torch.nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        groups=3\n",
    "    )\n",
    "    \n",
    "    output = conv(img.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    output_norm = (output - output.min()) / (output.max() - output.min())\n",
    "    \n",
    "    axes[idx + 1].imshow(output_norm.permute(1, 2, 0).detach().numpy())\n",
    "    axes[idx + 1].set_title(f'Stride={stride}\\n{output.shape[1]}Ã—{output.shape[2]}', \n",
    "                           fontsize=11, fontweight='bold')\n",
    "    axes[idx + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Larger strides make the output smaller - notice how the dimensions shrink!\")\n",
    "print(\"This helps reduce computation while keeping important COLOUR patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1YK1j14eswy"
   },
   "source": [
    "### 2d: Padding\n",
    "\n",
    "**Padding** adds a border around the image so we don't lose the edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "OJum221Ye7qU",
     "outputId": "cc9113f3-957f-4d0f-979c-43d3f60beee0"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "OJum221Ye7qU",
     "outputId": "14fbb604-1599-4d19-ba04-61b6fa07fffd"
    }
   },
   "outputs": [],
   "source": [
    "# Without padding\n",
    "conv_no_padding = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, padding=0, groups=3)\n",
    "output_no_padding = conv_no_padding(img.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# With padding\n",
    "conv_with_padding = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, padding=2, groups=3)\n",
    "output_with_padding = conv_with_padding(img.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(f'Original\\n{img.shape[1]}Ã—{img.shape[2]}', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Normalize outputs for visualization\n",
    "out_no_pad_norm = (output_no_padding - output_no_padding.min()) / (output_no_padding.max() - output_no_padding.min())\n",
    "axes[1].imshow(out_no_pad_norm.permute(1, 2, 0).detach().numpy())\n",
    "axes[1].set_title(f'No Padding\\n{output_no_padding.shape[1]}Ã—{output_no_padding.shape[2]}', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "out_with_pad_norm = (output_with_padding - output_with_padding.min()) / (output_with_padding.max() - output_with_padding.min())\n",
    "axes[2].imshow(out_with_pad_norm.permute(1, 2, 0).detach().numpy())\n",
    "axes[2].set_title(f'With Padding\\n{output_with_padding.shape[1]}Ã—{output_with_padding.shape[2]}', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Padding helps preserve image size and edge information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiFpaoXDe942"
   },
   "source": [
    "### 2e: dilation\n",
    "Define a convolutional kernel with size 7x7 and stride 1, and dilation 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "9mM0KfzJe9Z4",
     "outputId": "af471eb4-721d-4e28-b235-776a7d827625"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "9mM0KfzJe9Z4",
     "outputId": "90788115-3a9c-4770-e2de-74e2f68185cf"
    }
   },
   "outputs": [],
   "source": [
    "# Without dilation\n",
    "conv_no_dilation = torch.nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    kernel_size=3,\n",
    "    padding=1,      # keep spatial size\n",
    "    dilation=1,\n",
    "    groups=3\n",
    ")\n",
    "output_no_dilation = conv_no_dilation(img.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# With dilation\n",
    "conv_with_dilation = torch.nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    kernel_size=3,\n",
    "    padding=2,      # larger padding to match dilation\n",
    "    dilation=2,\n",
    "    groups=3\n",
    ")\n",
    "output_with_dilation = conv_with_dilation(img.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(f'Original\\n{img.shape[1]}Ã—{img.shape[2]}', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Normalize outputs for visualization\n",
    "out_no_dil_norm = (output_no_dilation - output_no_dilation.min()) / (\n",
    "    output_no_dilation.max() - output_no_dilation.min()\n",
    ")\n",
    "axes[1].imshow(out_no_dil_norm.permute(1, 2, 0).detach().numpy())\n",
    "axes[1].set_title(\n",
    "    f'No Dilation (d=1)\\n{output_no_dilation.shape[1]}Ã—{output_no_dilation.shape[2]}',\n",
    "    fontsize=11, fontweight='bold'\n",
    ")\n",
    "axes[1].axis('off')\n",
    "\n",
    "out_with_dil_norm = (output_with_dilation - output_with_dilation.min()) / (\n",
    "    output_with_dilation.max() - output_with_dilation.min()\n",
    ")\n",
    "axes[2].imshow(out_with_dil_norm.permute(1, 2, 0).detach().numpy())\n",
    "axes[2].set_title(\n",
    "    f'With Dilation (d=2)\\n{output_with_dilation.shape[1]}Ã—{output_with_dilation.shape[2]}',\n",
    "    fontsize=11, fontweight='bold'\n",
    ")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Dilation increases the receptive field without reducing resolution!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jwsESxPfalJ"
   },
   "source": [
    "## Exercise 3: Building a Complete CNN for MNIST\n",
    "\n",
    "Now let's put it all together and build a complete Convolutional Neural Network (CNN) that can recognize handwritten digits!\n",
    "\n",
    "### What is a CNN?\n",
    "\n",
    "A CNN is like a stack of filters that learns to detect patterns:\n",
    "1. **First layers**: Detect simple patterns (edges, curves)\n",
    "2. **Middle layers**: Combine patterns into parts (loops, lines)\n",
    "3. **Last layers**: Recognize complete objects (digits 0-9)\n",
    "\n",
    "Let's build one step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 178
     },
     "id": "m0Y5AHudCsFS",
     "outputId": "20b2d696-96b4-4cb8-ef4b-1096537076ce"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 177
     },
     "id": "m0Y5AHudCsFS",
     "outputId": "dd29119e-55e0-42c7-8de1-785e08d1ade7"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A CNN for recognizing MNIST digits\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer: takes 3 colour channels, finds 16 patterns\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,      # COLOUR INPUT (R, G, B)\n",
    "            out_channels=16,    # Learn 16 different filters\n",
    "            kernel_size=3,      # Each filter is 3Ã—3\n",
    "            padding=1           # Keep the same size\n",
    "        )\n",
    "        \n",
    "        # Second convolutional layer: finds 32 more complex patterns\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,     # Takes the 16 patterns from conv1\n",
    "            out_channels=32,    # Creates 32 new patterns\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        # Pooling layer: makes the image smaller\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers: make the final decision\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 outputs (digits 0-9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Process a COLOUR image through the network\"\"\"\n",
    "        # First conv layer + activation + pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # Second conv layer + activation + pooling  \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the image into a vector\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = SimpleCNN()\n",
    "print(\"Model created!\\n\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1wWObDjM0nl"
   },
   "source": [
    "### Step 2: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "Mw4AXzYzGyPx",
     "outputId": "07516fc4-0071-45b5-d255-6b8f7e142b84"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "Mw4AXzYzGyPx",
     "outputId": "bc5b70f0-eb4a-419e-b138-c3f8fb44afb5"
    }
   },
   "outputs": [],
   "source": [
    "# Custom dataset class for coloured MNIST\n",
    "class ColouredMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.mnist = torchvision.datasets.MNIST(\n",
    "            root=root,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        )\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gray_image, label = self.mnist[idx]\n",
    "        \n",
    "        # Colorize the image (use idx as seed for consistency)\n",
    "        colored_image = colourize_mnist(gray_image, colour_seed=idx)\n",
    "        \n",
    "        if self.transform:\n",
    "            colored_image = self.transform(colored_image)\n",
    "        \n",
    "        return colored_image, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ColouredMNIST(root='./data', train=True)\n",
    "test_dataset = # TODO: Create the test dataset using ColouredMNIST\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1000,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training images: {len(train_dataset):,}\")\n",
    "print(f\"Test images: {len(test_dataset):,}\")\n",
    "\n",
    "# Show some examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    img, label = train_dataset[i]\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(f'Digit: {label}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample MNIST Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LTl7hxdNvlF"
   },
   "source": [
    "### Step 3: Train the Model on Handwritten Digits\n",
    "\n",
    "Now let's teach our network to recognize digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22DbhSvheAeb"
   },
   "outputs": [],
   "source": [
    "# Set up training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Training on: {device}\")\n",
    "print(\"Starting training!\\n\")\n",
    "\n",
    "# Train for 3 epochs\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, '\n",
    "                  f'Loss: {train_loss/(batch_idx+1):.3f}, '\n",
    "                  f'Accuracy: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    # Test after each epoch\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            test_total += target.size(0)\n",
    "            test_correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    print(f'\\n Epoch {epoch+1} Summary:')\n",
    "    print(f'   Training Accuracy: {100.*correct/total:.2f}%')\n",
    "    print(f'   Test Accuracy: {100.*test_correct/test_total:.2f}%\\n')\n",
    "\n",
    "print(\"\\n Training complete!\")\n",
    "print(\"Our model can now recognize digits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYUdtLWiOaLv"
   },
   "source": [
    "### Step 4: Test Our Model\n",
    "\n",
    "Let's see how well our model can recognize digits it has never seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "nxvsG0XbVtgb",
     "outputId": "829942d9-04b7-4691-bd88-cbe18aee1b01"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "nxvsG0XbVtgb",
     "outputId": "e5b120a5-031a-459e-ba4d-cf36b15fc520"
    }
   },
   "outputs": [],
   "source": [
    "# Get some test images\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of test images\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i in range(10):\n",
    "    img, label = test_dataset[i]\n",
    "    test_images.append(img)\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Show predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    img = test_images[i].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img.unsqueeze(0))\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted = output.argmax(dim=1).item()\n",
    "        confidence = probabilities[0][predicted].item() * 100\n",
    "    \n",
    "    # Display\n",
    "    axes[i].imshow(test_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "    \n",
    "    # Colour: green if correct, red if wrong\n",
    "    actual = test_labels[i]\n",
    "    colour = 'green' if predicted == actual else 'red'\n",
    "    \n",
    "    axes[i].set_title(f'Predicted: {predicted} ({confidence:.0f}%)\\nActual: {actual}', \n",
    "                     color=colour, fontsize=10, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Predictions on Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing What the Network Learned from Colour\n",
    "\n",
    "Let's see what patterns the network learned in its first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "DXFWjWbhyhKX",
     "outputId": "02e8eca6-a455-4bea-cff5-7e9391ee51b9"
    },
    "remote_metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/"
     },
     "id": "DXFWjWbhyhKX",
     "outputId": "3a8aa4ef-6241-4601-ec1d-78767d19eb27"
    }
   },
   "outputs": [],
   "source": [
    "# Get the filters from the first convolutional layer\n",
    "filters = model.conv1.weight.data.cpu()\n",
    "\n",
    "print(f\"First layer filter shape: {filters.shape}\")\n",
    "print(f\"  â†’ {filters.shape[0]} filters, each with {filters.shape[1]} input channels (RGB)\")\n",
    "\n",
    "# Visualize them - each filter processes all 3 color channels\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(16):\n",
    "    # Each filter has 3 channels (R, G, B)\n",
    "    filter_rgb = filters[i]  # Shape: [3, 3, 3]\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    filter_rgb = (filter_rgb - filter_rgb.min()) / (filter_rgb.max() - filter_rgb.min())\n",
    "    \n",
    "    # Display as a color image\n",
    "    axes[i].imshow(filter_rgb.permute(1, 2, 0).numpy())\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Filter {i+1}\\n(RGB)', fontsize=9)\n",
    "\n",
    "plt.suptitle('Filters Learned by First Layer (Processing RGB)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These filters learned to detect patterns in colour!\")\n",
    "print(\"Each filter combines information from Red, Green, and Blue channels.\")\n",
    "print(\"The network figured out which colour combinations are useful for recognizing digits!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
